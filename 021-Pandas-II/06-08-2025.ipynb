{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cab84b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date: 06-08-2025: \n",
    "# Topic: Pandas-II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5eca96a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297bc406",
   "metadata": {},
   "source": [
    "##                                                    Pandas- Day II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1faa87d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Titanic Dataset (Data Science Dojo): https://raw.githubusercontent.com/datasciencedojo/datasets/refs/heads/master/titanic.csv\n",
    "Iris Dataset (Seaborn Data): https://raw.githubusercontent.com/mwaskom/seaborn-data/refs/heads/master/iris.csv\n",
    "Kaggle Titanic Link: https://www.kaggle.com/datasets/yasserh/titanic-dataset/data\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8564a940",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_dataset_github_url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/refs/heads/master/titanic.csv\"\n",
    "iris_dataset_github_url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/refs/heads/master/iris.csv\"\n",
    "kaggle_link = \"https://www.kaggle.com/datasets/yasserh/titanic-dataset/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2649c908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titanic dataset loaded successfully!\n",
      "\n",
      "--- First 3 Rows and First 2 Columns using .iloc ---\n",
      "   PassengerId  Survived\n",
      "0            1         0\n",
      "1            2         1\n",
      "2            3         1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the URL for the Titanic dataset\n",
    "titanic_dataset_github_url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/refs/heads/master/titanic.csv\"\n",
    "\n",
    "# Read the CSV file into a Pandas DataFrame\n",
    "try:\n",
    "    df_titanic = pd.read_csv(titanic_dataset_github_url)\n",
    "    print(\"Titanic dataset loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading CSV file: {e}\")\n",
    "    print(\"Please check the URL or your internet connection.\")\n",
    "\n",
    "# --- Extracting Data with .iloc ---\n",
    "\n",
    "# .iloc[row_selection, column_selection]\n",
    "# Remember: .iloc uses integer-location based indexing, and slices are EXCLUSIVE of the end.\n",
    "\n",
    "# To get the first 3 rows, we use slicing from 0 up to (but not including) 3: 0:3\n",
    "# To get the first 2 columns, we use slicing from 0 up to (but not including) 2: 0:2\n",
    "\n",
    "extracted_data = df_titanic.iloc[0:3, 0:2]\n",
    "\n",
    "print(\"\\n--- First 3 Rows and First 2 Columns using .iloc ---\")\n",
    "print(extracted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09213ae9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Pclass\n",
       "0            1       3\n",
       "1            2       1\n",
       "2            3       3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_titanic.iloc[0:3, [0, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ee706ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>PassengerId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pclass  PassengerId\n",
       "0       3            1\n",
       "1       1            2\n",
       "2       3            3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_titanic.iloc[0:3, [2, 0]] # swap column order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68a3d4e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88551901",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The General Rule of Thumb for loc and iloc\n",
    "In Pandas:\n",
    "\n",
    "iloc (integer-location based indexing): When you use slicing with iloc (e.g., df.iloc[0:5, 0:2]), \n",
    "it generally tries to return a view if possible, for efficiency. \n",
    "If you use fancy indexing (e.g., df.iloc[[0, 5], [0, 2]]), it will typically return a copy.\n",
    "\n",
    "\n",
    "loc (label-based indexing): Similar to iloc, when you use slicing with loc (e.g., df.loc['row_start':'row_end', 'col_start':'col_end']),\n",
    "it generally tries to return a view. If you use fancy indexing (e.g., df.loc[['row_label1', 'row_label2'], ['col_label1', 'col_label2']])\n",
    " or boolean indexing, it will typically return a copy.\n",
    " \n",
    "The behavior of returning a view versus a copy can sometimes be complex and depends on internal optimizations in Pandas. It's not always guaranteed, and Pandas sometimes gives you a SettingWithCopyWarning to alert you to potential issues when you might be modifying a view that was intended to be a copy.\n",
    "\n",
    "Why is this distinction important?\n",
    "The core issue is whether modifying the returned object will also modify the original DataFrame.\n",
    "\n",
    "If it's a View: Modifying the view will also modify the original DataFrame.\n",
    "If it's a Copy: Modifying the copy will NOT modify the original DataFrame.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ea36b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''You want a COPY when:\n",
    "\n",
    "You intend to make changes to the selected subset of data, and you do not want those changes to affect the original DataFrame. This is the most common scenario for data analysis and cleaning, where you create intermediate processed versions of your data.\n",
    "You are performing an operation that Pandas knows will result in a fragmented or non-contiguous memory layout if it were a view, so it defaults to a copy for performance and consistency.\n",
    "You are okay with (or even want) a VIEW when:\n",
    "\n",
    "You are only reading data from the subset and don't intend to modify it.\n",
    "You specifically want to make changes to a subset of the original DataFrame in-place and are aware that the original will be affected. This can be more memory-efficient if you don't need a separate copy.\n",
    "How to Guarantee a Copy\n",
    "Because the view/copy behavior can sometimes be tricky, the best practice when you definitely need a copy (which is most of the time you're extracting data for separate manipulation) is to explicitly use the .copy() method:\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5098c6bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original df:\n",
      "    col1  col2\n",
      "0     1     4\n",
      "1     2     5\n",
      "2     3     6\n",
      "\n",
      "Original df after modifying copies (should be unchanged):\n",
      "    col1  col2\n",
      "0     1     4\n",
      "1     2     5\n",
      "2     3     6\n",
      "\n",
      "subset_copy_loc:\n",
      "    col1\n",
      "0   999\n",
      "2     3\n",
      "\n",
      "subset_copy_iloc:\n",
      "    col1  col2\n",
      "0   888     4\n",
      "1     2     5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {'col1': [1, 2, 3], 'col2': [4, 5, 6]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# This will be a copy, regardless of internal Pandas heuristics\n",
    "subset_copy_loc = df.loc[[0, 2], ['col1']].copy()\n",
    "subset_copy_iloc = df.iloc[0:2, :].copy() # Even though slicing might be a view, .copy() forces it\n",
    "\n",
    "print(\"Original df:\\n\", df)\n",
    "\n",
    "subset_copy_loc.iloc[0, 0] = 999\n",
    "subset_copy_iloc.iloc[0, 0] = 888\n",
    "\n",
    "print(\"\\nOriginal df after modifying copies (should be unchanged):\\n\", df)\n",
    "print(\"\\nsubset_copy_loc:\\n\", subset_copy_loc)\n",
    "print(\"\\nsubset_copy_iloc:\\n\", subset_copy_iloc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2bf1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "As you can see, the df remains unchanged because we explicitly used .copy().\n",
    "\n",
    "Summary:\n",
    "iloc and loc (with slicing): Tend to return views, but not always guaranteed.\n",
    "iloc and loc (with fancy indexing/boolean indexing): Tend to return copies.\n",
    "When in doubt, or when you explicitly need to modify the subset without \n",
    "affecting the original, use .copy() after your selection. \n",
    "This is the safest and clearest approach to ensure you're working with independent data.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b090990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titanic dataset loaded successfully!\n",
      "\n",
      "--- Original df_titanic Head (Default Index) ---\n",
      "   PassengerId  Survived  Pclass  \\\n",
      "0            1         0       3   \n",
      "1            2         1       1   \n",
      "2            3         1       3   \n",
      "3            4         1       1   \n",
      "4            5         0       3   \n",
      "\n",
      "                                                Name     Sex   Age  SibSp  \\\n",
      "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
      "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
      "4                           Allen, Mr. William Henry    male  35.0      0   \n",
      "\n",
      "   Parch            Ticket     Fare Cabin Embarked  \n",
      "0      0         A/5 21171   7.2500   NaN        S  \n",
      "1      0          PC 17599  71.2833   C85        C  \n",
      "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
      "3      0            113803  53.1000  C123        S  \n",
      "4      0            373450   8.0500   NaN        S  \n",
      "\n",
      "--- df_by_name Head (Name as Index) ---\n",
      "                                                    PassengerId  Survived  \\\n",
      "Name                                                                        \n",
      "Braund, Mr. Owen Harris                                       1         0   \n",
      "Cumings, Mrs. John Bradley (Florence Briggs Tha...            2         1   \n",
      "Heikkinen, Miss. Laina                                        3         1   \n",
      "Futrelle, Mrs. Jacques Heath (Lily May Peel)                  4         1   \n",
      "Allen, Mr. William Henry                                      5         0   \n",
      "\n",
      "                                                    Pclass     Sex   Age  \\\n",
      "Name                                                                       \n",
      "Braund, Mr. Owen Harris                                  3    male  22.0   \n",
      "Cumings, Mrs. John Bradley (Florence Briggs Tha...       1  female  38.0   \n",
      "Heikkinen, Miss. Laina                                   3  female  26.0   \n",
      "Futrelle, Mrs. Jacques Heath (Lily May Peel)             1  female  35.0   \n",
      "Allen, Mr. William Henry                                 3    male  35.0   \n",
      "\n",
      "                                                    SibSp  Parch  \\\n",
      "Name                                                               \n",
      "Braund, Mr. Owen Harris                                 1      0   \n",
      "Cumings, Mrs. John Bradley (Florence Briggs Tha...      1      0   \n",
      "Heikkinen, Miss. Laina                                  0      0   \n",
      "Futrelle, Mrs. Jacques Heath (Lily May Peel)            1      0   \n",
      "Allen, Mr. William Henry                                0      0   \n",
      "\n",
      "                                                              Ticket     Fare  \\\n",
      "Name                                                                            \n",
      "Braund, Mr. Owen Harris                                    A/5 21171   7.2500   \n",
      "Cumings, Mrs. John Bradley (Florence Briggs Tha...          PC 17599  71.2833   \n",
      "Heikkinen, Miss. Laina                              STON/O2. 3101282   7.9250   \n",
      "Futrelle, Mrs. Jacques Heath (Lily May Peel)                  113803  53.1000   \n",
      "Allen, Mr. William Henry                                      373450   8.0500   \n",
      "\n",
      "                                                   Cabin Embarked  \n",
      "Name                                                               \n",
      "Braund, Mr. Owen Harris                              NaN        S  \n",
      "Cumings, Mrs. John Bradley (Florence Briggs Tha...   C85        C  \n",
      "Heikkinen, Miss. Laina                               NaN        S  \n",
      "Futrelle, Mrs. Jacques Heath (Lily May Peel)        C123        S  \n",
      "Allen, Mr. William Henry                             NaN        S  \n",
      "\n",
      "Type of df_by_name index: <class 'pandas.core.indexes.base.Index'>\n",
      "Name of df_by_name index: Name\n"
     ]
    }
   ],
   "source": [
    "# setting index by a feature of interest:\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Define the URL for the Titanic dataset\n",
    "titanic_dataset_github_url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/refs/heads/master/titanic.csv\"\n",
    "\n",
    "# Read the CSV file into a Pandas DataFrame\n",
    "try:\n",
    "    df_titanic = pd.read_csv(titanic_dataset_github_url)\n",
    "    print(\"Titanic dataset loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading CSV file: {e}\")\n",
    "    print(\"Please check the URL or your internet connection.\")\n",
    "\n",
    "# Set the 'Name' column as the new index\n",
    "df_by_name = df_titanic.set_index('Name')\n",
    "\n",
    "print(\"\\n--- Original df_titanic Head (Default Index) ---\")\n",
    "print(df_titanic.head())\n",
    "\n",
    "print(\"\\n--- df_by_name Head (Name as Index) ---\")\n",
    "print(df_by_name.head())\n",
    "\n",
    "print(f\"\\nType of df_by_name index: {type(df_by_name.index)}\")\n",
    "print(f\"Name of df_by_name index: {df_by_name.index.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7e9784",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "What df_titanic.set_index('Name') does:\n",
    "\n",
    "Creates a New DataFrame: By default, set_index() returns a new DataFrame (df_by_name in this case) \n",
    "and does not modify the original df_titanic in place. If you wanted to modify df_titanic directly, \n",
    "you would use df_titanic.set_index('Name', inplace=True).\n",
    "\n",
    "Sets the Index: The values from the specified column ('Name') are removed from their column position \n",
    "and are used to create a new index for the DataFrame.\n",
    "\n",
    "Unique Labels: Each passenger's name now acts as a unique label for their corresponding row. \n",
    "This makes it very intuitive to look up information for a specific person directly by their name.\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abad85d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PassengerId            1\n",
      "Survived               0\n",
      "Pclass                 3\n",
      "Sex                 male\n",
      "Age                 22.0\n",
      "SibSp                  1\n",
      "Parch                  0\n",
      "Ticket         A/5 21171\n",
      "Fare                7.25\n",
      "Cabin                NaN\n",
      "Embarked               S\n",
      "Name: Braund, Mr. Owen Harris, dtype: object\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "Why this is useful:\n",
    "Label-based Lookup: You can now use .loc[] to directly access rows by passenger names, \n",
    "which is often more intuitive than remembering their original integer row number.\n",
    "\n",
    "'''\n",
    "\n",
    "# Example: Get data for 'Braund, Mr. Owen Harris' using the new index\n",
    "print(df_by_name.loc['Braund, Mr. Owen Harris'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c295e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Meaningful Access: For datasets where a specific column naturally serves as an identifier, \n",
    "setting it as the index provides a more semantic way to interact with your data.\n",
    "\n",
    "Joining/Merging: When combining DataFrames, having common columns as indices can often simplify\n",
    " merge operations.\n",
    "\n",
    "Now that df_by_name has 'Name' as its index, you can perform label-based lookups much more \n",
    "easily for individual passengers!\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7285c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .reset_index()\n",
    "\n",
    "# How to reverse the set_index() operation, bringing the column that was promoted to the index back into\n",
    "# the DataFrame as a regular data column, and reverting to the default numerical (positional) index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc123e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titanic dataset loaded successfully!\n",
      "\n",
      "--- df_by_name (Name as Index) Head ---\n",
      "                                                    PassengerId  Survived  \\\n",
      "Name                                                                        \n",
      "Braund, Mr. Owen Harris                                       1         0   \n",
      "Cumings, Mrs. John Bradley (Florence Briggs Tha...            2         1   \n",
      "Heikkinen, Miss. Laina                                        3         1   \n",
      "Futrelle, Mrs. Jacques Heath (Lily May Peel)                  4         1   \n",
      "Allen, Mr. William Henry                                      5         0   \n",
      "\n",
      "                                                    Pclass     Sex   Age  \\\n",
      "Name                                                                       \n",
      "Braund, Mr. Owen Harris                                  3    male  22.0   \n",
      "Cumings, Mrs. John Bradley (Florence Briggs Tha...       1  female  38.0   \n",
      "Heikkinen, Miss. Laina                                   3  female  26.0   \n",
      "Futrelle, Mrs. Jacques Heath (Lily May Peel)             1  female  35.0   \n",
      "Allen, Mr. William Henry                                 3    male  35.0   \n",
      "\n",
      "                                                    SibSp  Parch  \\\n",
      "Name                                                               \n",
      "Braund, Mr. Owen Harris                                 1      0   \n",
      "Cumings, Mrs. John Bradley (Florence Briggs Tha...      1      0   \n",
      "Heikkinen, Miss. Laina                                  0      0   \n",
      "Futrelle, Mrs. Jacques Heath (Lily May Peel)            1      0   \n",
      "Allen, Mr. William Henry                                0      0   \n",
      "\n",
      "                                                              Ticket     Fare  \\\n",
      "Name                                                                            \n",
      "Braund, Mr. Owen Harris                                    A/5 21171   7.2500   \n",
      "Cumings, Mrs. John Bradley (Florence Briggs Tha...          PC 17599  71.2833   \n",
      "Heikkinen, Miss. Laina                              STON/O2. 3101282   7.9250   \n",
      "Futrelle, Mrs. Jacques Heath (Lily May Peel)                  113803  53.1000   \n",
      "Allen, Mr. William Henry                                      373450   8.0500   \n",
      "\n",
      "                                                   Cabin Embarked  \n",
      "Name                                                               \n",
      "Braund, Mr. Owen Harris                              NaN        S  \n",
      "Cumings, Mrs. John Bradley (Florence Briggs Tha...   C85        C  \n",
      "Heikkinen, Miss. Laina                               NaN        S  \n",
      "Futrelle, Mrs. Jacques Heath (Lily May Peel)        C123        S  \n",
      "Allen, Mr. William Henry                             NaN        S  \n",
      "df_by_name index type: <class 'pandas.core.indexes.base.Index'>\n",
      "Is 'Name' a column in df_by_name? False\n",
      "\n",
      "--- df_reset (After reset_index() - Name is now a column) Head ---\n",
      "                                                Name  PassengerId  Survived  \\\n",
      "0                            Braund, Mr. Owen Harris            1         0   \n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...            2         1   \n",
      "2                             Heikkinen, Miss. Laina            3         1   \n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)            4         1   \n",
      "4                           Allen, Mr. William Henry            5         0   \n",
      "\n",
      "   Pclass     Sex   Age  SibSp  Parch            Ticket     Fare Cabin  \\\n",
      "0       3    male  22.0      1      0         A/5 21171   7.2500   NaN   \n",
      "1       1  female  38.0      1      0          PC 17599  71.2833   C85   \n",
      "2       3  female  26.0      0      0  STON/O2. 3101282   7.9250   NaN   \n",
      "3       1  female  35.0      1      0            113803  53.1000  C123   \n",
      "4       3    male  35.0      0      0            373450   8.0500   NaN   \n",
      "\n",
      "  Embarked  \n",
      "0        S  \n",
      "1        C  \n",
      "2        S  \n",
      "3        S  \n",
      "4        S  \n",
      "df_reset index type: <class 'pandas.core.indexes.range.RangeIndex'>\n",
      "Is 'Name' a column in df_reset? True\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the URL for the Titanic dataset\n",
    "titanic_dataset_github_url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/refs/heads/master/titanic.csv\"\n",
    "\n",
    "# Read the CSV file into a Pandas DataFrame\n",
    "try:\n",
    "    df_titanic = pd.read_csv(titanic_dataset_github_url)\n",
    "    print(\"Titanic dataset loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading CSV file: {e}\")\n",
    "    print(\"Please check the URL or your internet connection.\")\n",
    "\n",
    "# 1. Create df_by_name with 'Name' as the index (as you did previously)\n",
    "df_by_name = df_titanic.set_index('Name')\n",
    "\n",
    "print(\"\\n--- df_by_name (Name as Index) Head ---\")\n",
    "print(df_by_name.head())\n",
    "print(f\"df_by_name index type: {type(df_by_name.index)}\")\n",
    "print(f\"Is 'Name' a column in df_by_name? {'Name' in df_by_name.columns}\")\n",
    "\n",
    "# 2. Convert 'Name' back to a data column and reset to default integer index\n",
    "# By default, reset_index() will convert the current index into a column\n",
    "# and create a new default integer index (0, 1, 2, ...)\n",
    "df_reset = df_by_name.reset_index()\n",
    "\n",
    "print(\"\\n--- df_reset (After reset_index() - Name is now a column) Head ---\")\n",
    "print(df_reset.head())\n",
    "print(f\"df_reset index type: {type(df_reset.index)}\")\n",
    "print(f\"Is 'Name' a column in df_reset? {'Name' in df_reset.columns}\")\n",
    "\n",
    "# You can also verify that df_reset is identical to the original df_titanic (except for potential memory addresses)\n",
    "# print(\"\\nAre df_titanic and df_reset identical (ignoring memory)?\")\n",
    "# print(df_titanic.equals(df_reset)) # This should return True if no other operations were done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c36178cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- January Sales ---\n",
      "Store A    1500\n",
      "Store B    2000\n",
      "Store C    1800\n",
      "Name: January Sales, dtype: int64\n",
      "------------------------------\n",
      "\n",
      "--- February Sales (Store B is missing) ---\n",
      "Store A    1700\n",
      "Store C    1950\n",
      "Name: February Sales, dtype: int64\n",
      "------------------------------\n",
      "\n",
      "--- February Sales Reindexed (using common_index and fill_value=0) ---\n",
      "Store A    1700\n",
      "Store B       0\n",
      "Store C    1950\n",
      "Name: February Sales, dtype: int64\n",
      "------------------------------\n",
      "\n",
      "--- Sales Difference (February - January) ---\n",
      "Store A     200\n",
      "Store B   -2000\n",
      "Store C     150\n",
      "dtype: int64\n",
      "------------------------------\n",
      "\n",
      "--- Interpretation of Results ---\n",
      "Store A: Increased sales by $200.00\n",
      "Store B: Sales decreased by $2000.00 (due to closure)\n",
      "Store C: Increased sales by $150.00\n"
     ]
    }
   ],
   "source": [
    "# .reindex\n",
    "# ===============\n",
    "import pandas as pd\n",
    "\n",
    "# --- 1. Define Sales Data for January and February ---\n",
    "\n",
    "# January Sales Data: Stores A, B, C\n",
    "jan_sales_data = {\n",
    "    'Store A': 1500,\n",
    "    'Store B': 2000,\n",
    "    'Store C': 1800\n",
    "}\n",
    "s_jan = pd.Series(jan_sales_data, name='January Sales')\n",
    "\n",
    "print(\"--- January Sales ---\")\n",
    "print(s_jan)\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# February Sales Data: Store B was closed, so no sales entry for it\n",
    "feb_sales_data = {\n",
    "    'Store A': 1700,\n",
    "    'Store C': 1950\n",
    "}\n",
    "s_feb = pd.Series(feb_sales_data, name='February Sales')\n",
    "\n",
    "print(\"\\n--- February Sales (Store B is missing) ---\")\n",
    "print(s_feb)\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# --- 2. Define a Common Index ---\n",
    "# This index includes all stores present across both months\n",
    "common_index = ['Store A', 'Store B', 'Store C']\n",
    "\n",
    "# --- 3. Reindex February Sales to be Compatible with January Sales ---\n",
    "# We use reindex() to align s_feb with the common_index.\n",
    "# For any index label in common_index that is NOT in s_feb (like 'Store B'),\n",
    "# fill its value with 0, because the store was closed.\n",
    "s_feb_reindexed = s_feb.reindex(common_index, fill_value=0)\n",
    "\n",
    "print(f\"\\n--- February Sales Reindexed (using common_index and fill_value=0) ---\")\n",
    "print(s_feb_reindexed)\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# --- 4. Find the Difference in Sales Accurately Between Two Months ---\n",
    "# Now that both Series have the same index and missing values are handled (filled with 0),\n",
    "# we can perform direct element-wise subtraction.\n",
    "sales_difference = s_feb_reindexed - s_jan\n",
    "\n",
    "print(f\"\\n--- Sales Difference (February - January) ---\")\n",
    "print(sales_difference)\n",
    "print(\"-\" * 30)\n",
    "\n",
    "print(\"\\n--- Interpretation of Results ---\")\n",
    "print(f\"Store A: Increased sales by ${sales_difference['Store A']:.2f}\")\n",
    "print(f\"Store B: Sales decreased by ${abs(sales_difference['Store B']):.2f} (due to closure)\")\n",
    "print(f\"Store C: Increased sales by ${sales_difference['Store C']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccdcc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Explanation:\n",
    "Initial Series (s_jan, s_feb): Notice that s_feb naturally doesn't have an entry for \n",
    "'Store B' because it was closed.\n",
    "Attempting Direct Subtraction (Without reindex): If you tried s_feb - s_jan directly, \n",
    "Pandas would align them by index. For 'Store B', since it exists in s_jan but not s_feb, \n",
    "it would result in NaN in the difference, which isn't what we want for \"closed\" sales (we want 0).\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ee0613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What would happen without reindex() for Store B:\n",
    "# print(s_feb - s_jan)\n",
    "# Store A     200.0\n",
    "# Store B     NaN  <-- Problem!\n",
    "# Store C     150.0\n",
    "# dtype: float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f568258",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "common_index: We explicitly define all the labels we expect to see in our final aligned data.\n",
    "\n",
    "\n",
    "s_feb.reindex(common_index, fill_value=0):\n",
    "This creates a new Series based on s_feb.\n",
    "It aligns s_feb to common_index.\n",
    "For 'Store B', which is in common_index but not in s_feb, reindex inserts this label and\n",
    "assigns it the fill_value of 0. This accurately represents \"zero sales\" for the closed store.\n",
    "Existing labels ('Store A', 'Store C') retain their values.\n",
    "\n",
    "\n",
    "sales_difference = s_feb_reindexed - s_jan: Now, both s_feb_reindexed and s_jan have the exact same index (['Store A', 'Store B', 'Store C']), allowing for a correct element-wise subtraction that accounts for Store B's closure as zero sales.\n",
    "This example clearly shows how reindex() with fill_value is essential for performing accurate calculations across datasets that might have misaligned or missing entries for certain labels.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f348fa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  what if January has store A, B and C but Feb has stores A, C and D.\n",
    "\n",
    "'''\n",
    "\n",
    "Okay, this is a more complex and very realistic scenario for data alignment! When \n",
    "neither dataset contains all the unique labels present across both, you need to reindex \n",
    "both Series (or DataFrames) to a comprehensive common index.\n",
    "\n",
    "Here's how to handle it:\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f4d1db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- January Sales ---\n",
      "Store A    1500\n",
      "Store B    2000\n",
      "Store C    1800\n",
      "Name: January Sales, dtype: int64\n",
      "------------------------------\n",
      "\n",
      "--- February Sales (Store B is missing, Store D is new) ---\n",
      "Store A    1700\n",
      "Store C    1950\n",
      "Store D    1200\n",
      "Name: February Sales, dtype: int64\n",
      "------------------------------\n",
      "\n",
      "--- Common Index for both months ---\n",
      "['Store A', 'Store B', 'Store C', 'Store D']\n",
      "------------------------------\n",
      "\n",
      "--- January Sales Reindexed (with Store D filled as 0) ---\n",
      "Store A    1500\n",
      "Store B    2000\n",
      "Store C    1800\n",
      "Store D       0\n",
      "Name: January Sales, dtype: int64\n",
      "------------------------------\n",
      "\n",
      "--- February Sales Reindexed (with Store B filled as 0) ---\n",
      "Store A    1700\n",
      "Store B       0\n",
      "Store C    1950\n",
      "Store D    1200\n",
      "Name: February Sales, dtype: int64\n",
      "------------------------------\n",
      "\n",
      "--- Sales Difference (February - January) ---\n",
      "Store A     200\n",
      "Store B   -2000\n",
      "Store C     150\n",
      "Store D    1200\n",
      "dtype: int64\n",
      "------------------------------\n",
      "\n",
      "--- Interpretation of Results ---\n",
      "Store A: Increased sales by $200.00\n",
      "Store B: Sales decreased by $2000.00 (closed in Feb)\n",
      "Store C: Increased sales by $150.00\n",
      "Store D: Increased sales by $1200.00 (new in Feb)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- 1. Define Sales Data for January and February ---\n",
    "\n",
    "# January Sales Data: Stores A, B, C\n",
    "jan_sales_data = {\n",
    "    'Store A': 1500,\n",
    "    'Store B': 2000,\n",
    "    'Store C': 1800\n",
    "}\n",
    "s_jan = pd.Series(jan_sales_data, name='January Sales')\n",
    "\n",
    "print(\"--- January Sales ---\")\n",
    "print(s_jan)\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# February Sales Data: Stores A, C, D (Store B is gone, Store D is new)\n",
    "feb_sales_data = {\n",
    "    'Store A': 1700,\n",
    "    'Store C': 1950,\n",
    "    'Store D': 1200 # New store D\n",
    "}\n",
    "s_feb = pd.Series(feb_sales_data, name='February Sales')\n",
    "\n",
    "print(\"\\n--- February Sales (Store B is missing, Store D is new) ---\")\n",
    "print(s_feb)\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# --- 2. Determine the Common Index ---\n",
    "# The common index should be the union of all store names from both months.\n",
    "# We can get this by taking the union of their indices.\n",
    "common_index = s_jan.index.union(s_feb.index)\n",
    "\n",
    "print(f\"\\n--- Common Index for both months ---\")\n",
    "print(common_index.tolist()) # Convert to list for cleaner printing\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# --- 3. Reindex BOTH January and February Sales to the Common Index ---\n",
    "# For any store that didn't exist in a given month, its sales should be 0.\n",
    "\n",
    "s_jan_reindexed = s_jan.reindex(common_index, fill_value=0)\n",
    "s_feb_reindexed = s_feb.reindex(common_index, fill_value=0)\n",
    "\n",
    "print(f\"\\n--- January Sales Reindexed (with Store D filled as 0) ---\")\n",
    "print(s_jan_reindexed)\n",
    "print(\"-\" * 30)\n",
    "\n",
    "print(f\"\\n--- February Sales Reindexed (with Store B filled as 0) ---\")\n",
    "print(s_feb_reindexed)\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# --- 4. Find the Difference in Sales Accurately Between Two Months ---\n",
    "# Now that both Series are perfectly aligned with the same index and 0s for missing data,\n",
    "# we can perform direct element-wise subtraction.\n",
    "sales_difference = s_feb_reindexed - s_jan_reindexed\n",
    "\n",
    "print(f\"\\n--- Sales Difference (February - January) ---\")\n",
    "print(sales_difference)\n",
    "print(\"-\" * 30)\n",
    "\n",
    "print(\"\\n--- Interpretation of Results ---\")\n",
    "print(f\"Store A: Increased sales by ${sales_difference['Store A']:.2f}\")\n",
    "print(f\"Store B: Sales decreased by ${abs(sales_difference['Store B']):.2f} (closed in Feb)\")\n",
    "print(f\"Store C: Increased sales by ${sales_difference['Store C']:.2f}\")\n",
    "print(f\"Store D: Increased sales by ${sales_difference['Store D']:.2f} (new in Feb)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245e6488",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Explanation of Changes:\n",
    "\n",
    "Modified s_feb: Now it explicitly includes 'Store D' and implicitly lacks 'Store B'.\n",
    "\n",
    "common_index = s_jan.index.union(s_feb.index): This is the crucial part.\n",
    "s_jan.index gives Index(['Store A', 'Store B', 'Store C'], dtype='object').\n",
    "s_feb.index gives Index(['Store A', 'Store C', 'Store D'], dtype='object').\n",
    "\n",
    "The .union() method on these indexes efficiently computes the unique labels that are present in \n",
    "either index, resulting in Index(['Store A', 'Store B', 'Store C', 'Store D'], dtype='object'). \n",
    "This creates our comprehensive common index.\n",
    "\n",
    "Reindex Both Series: We apply reindex(common_index, fill_value=0) to both s_jan and s_feb.\n",
    "s_jan_reindexed now has 'Store D' with a value of 0 (since it had no sales in Jan).\n",
    "s_feb_reindexed now has 'Store B' with a value of 0 (since it had no sales in Feb).\n",
    "\n",
    "Accurate Subtraction: With both Series now perfectly aligned and all relevant \"missing\" data \n",
    "explicitly represented as 0s, the subtraction s_feb_reindexed - s_jan_reindexed gives \n",
    "accurate differences for all stores, including those that appeared or disappeared.\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3866e2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reindexing with Columns:\n",
    "########################################\n",
    "\n",
    "\n",
    "'''\n",
    "Certainly! Reindexing isn't just for rows; it's equally powerful for aligning columns in DataFrames.\n",
    " This is particularly useful when you have different sets of columns in various DataFrames and \n",
    " want to combine or compare them consistently.\n",
    "\n",
    "Let's imagine you have product data where some products have 'Price' and 'Stock' information,\n",
    " and another DataFrame (perhaps from a different source) has similar data but with a different \n",
    " set of columns, or the columns are in a different order.\n",
    "\n",
    "Here's an example demonstrating reindex() with columns:\n",
    "\n",
    "Reindexing with Columns\n",
    "Imagine you have two DataFrames representing product data. df_current_month has 'Price' \n",
    "and 'Stock' for various products, but df_last_month only has 'Price' and introduces a 'Discount'\n",
    "column that isn't in the current month's data. Our goal is to align the columns of df_last_month \n",
    "to match df_current_month, filling in missing 'Stock' values with 0.\n",
    "\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# --- 1. Define Product DataFrames ---\n",
    "\n",
    "# Current month's product data\n",
    "data_current = {\n",
    "    'ProductID': ['P101', 'P102', 'P103', 'P104'],\n",
    "    'Price': [25.50, 12.00, 75.00, 5.25],\n",
    "    'Stock': [100, 50, 20, 200]\n",
    "}\n",
    "df_current_month = pd.DataFrame(data_current).set_index('ProductID')\n",
    "\n",
    "print(\"--- df_current_month (Current Month's Product Data) ---\")\n",
    "print(df_current_month)\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Last month's product data (missing 'Stock', has 'Discount')\n",
    "data_last = {\n",
    "    'ProductID': ['P101', 'P102', 'P103', 'P105'], # P105 is new, P104 is missing\n",
    "    'Price': [24.99, 11.50, 72.00, 4.99],\n",
    "    'Discount': [0.05, 0.02, 0.10, 0.0]\n",
    "}\n",
    "df_last_month = pd.DataFrame(data_last).set_index('ProductID')\n",
    "\n",
    "print(\"\\n--- df_last_month (Last Month's Product Data - Different Columns) ---\")\n",
    "print(df_last_month)\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# --- 2. Define the Desired Common Columns ---\n",
    "# We want df_last_month to have the same columns as df_current_month,\n",
    "# in the same order.\n",
    "desired_columns = ['Price', 'Stock']\n",
    "\n",
    "# --- 3. Reindex df_last_month's Columns ---\n",
    "# We'll reindex the columns of df_last_month to match `desired_columns`.\n",
    "# For any column in `desired_columns` that is NOT in df_last_month (like 'Stock'),\n",
    "# we'll fill its values with 0, as there was no stock recorded last month in this view.\n",
    "# Notice `axis=1` is specified to indicate column reindexing.\n",
    "df_last_month_reindexed_cols = df_last_month.reindex(columns=desired_columns, fill_value=0)\n",
    "\n",
    "print(f\"\\n--- df_last_month after reindexing columns to '{desired_columns}' ---\")\n",
    "print(df_last_month_reindexed_cols)\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# --- 4. Align Rows and Then Calculate Difference (Example) ---\n",
    "# Now that columns are compatible, you might want to align rows (ProductID) too\n",
    "# before doing calculations. Let's get a union of all ProductIDs.\n",
    "all_product_ids = df_current_month.index.union(df_last_month.index)\n",
    "\n",
    "df_current_aligned = df_current_month.reindex(all_product_ids, fill_value=0)\n",
    "df_last_aligned = df_last_month_reindexed_cols.reindex(all_product_ids, fill_value=0) # Use the col-reindexed one\n",
    "\n",
    "print(f\"\\n--- df_current_month aligned by all ProductIDs ---\")\n",
    "print(df_current_aligned)\n",
    "\n",
    "print(f\"\\n--- df_last_month (after both column and row reindexing) ---\")\n",
    "print(df_last_aligned)\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Now we can accurately calculate the change in price or stock\n",
    "price_change = df_current_aligned['Price'] - df_last_aligned['Price']\n",
    "stock_change = df_current_aligned['Stock'] - df_last_aligned['Stock']\n",
    "\n",
    "print(f\"\\n--- Price Change (Current Month - Last Month) ---\")\n",
    "print(price_change)\n",
    "\n",
    "print(f\"\\n--- Stock Change (Current Month - Last Month) ---\")\n",
    "print(stock_change)\n",
    "\n",
    "'''\n",
    "\n",
    "Explanation:\n",
    "\n",
    "\n",
    "Initial DataFrames:\n",
    "df_current_month has Price and Stock columns.\n",
    "df_last_month has Price and Discount columns (no Stock).\n",
    "desired_columns: We define the precise list of column names (and their order) \n",
    "that we want our target DataFrame (df_last_month) to have.\n",
    "\n",
    "df_last_month.reindex(columns=desired_columns, fill_value=0):\n",
    "columns=desired_columns: This is the key part that tells reindex to operate on the columns\n",
    " (instead of the default rows).\n",
    "\n",
    "fill_value=0: For any column in desired_columns that was not present in df_last_month\n",
    " (i.e., 'Stock'), Pandas creates that column and fills all its values with 0.\n",
    "Any columns in df_last_month that are not in desired_columns (i.e., 'Discount') \n",
    "are dropped by default.\n",
    "\n",
    "Full Alignment for Calculation: After reindexing the columns, I added a step to also\n",
    " reindex the rows (ProductID) of both DataFrames to a common set of IDs. This ensures \n",
    " that when you perform an arithmetic operation like subtraction, Pandas can perfectly \n",
    " align both the rows and columns, giving you accurate changes.\n",
    "This example clearly shows how reindex() with the columns parameter provides precise \n",
    "control over DataFrame column structure, making disparate datasets compatible for analysis.\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad045c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterrows\n",
    "#################\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "The df.iterrows() method in Pandas is a generator that iterates over the rows of a DataFrame as (index, Series) pairs. For each row, it returns the row's index and a Pandas Series containing the row's data.\n",
    "\n",
    "When to use iterrows()\n",
    "iterrows() is useful when you need to process each row of a DataFrame individually and access both its index and its data.\n",
    "\n",
    "Important Considerations (and why it's often not recommended)\n",
    "While iterrows() seems intuitive, it's generally not the most performant or \"Pandas-idiomatic\" way to iterate through rows, especially for large DataFrames. Here's why:\n",
    "\n",
    "Returns a Series for each row: Creating a Series for each row can be computationally expensive compared to other methods.\n",
    "Type conversion: The Series returned for each row might not retain the original dtype of the DataFrame columns. Pandas tries to infer the best common dtype for the entire row, which can lead to unexpected type changes (e.g., integers becoming floats if there's a NaN in a column).\n",
    "Performance: For large DataFrames, explicit Python loops with iterrows() are significantly slower than vectorized Pandas operations.\n",
    "Rule of Thumb: If you find yourself using iterrows() to perform calculations or manipulations across rows, first consider if there's a vectorized Pandas operation (e.g., column arithmetic, apply(), groupby(), agg(), transform()) that can achieve the same result more efficiently.\n",
    "\n",
    "Example Usage\n",
    "Let's use the Titanic dataset to demonstrate iterrows().\n",
    "\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Titanic dataset\n",
    "titanic_dataset_github_url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/refs/heads/master/titanic.csv\"\n",
    "try:\n",
    "    df_titanic = pd.read_csv(titanic_dataset_github_url)\n",
    "    print(\"Titanic dataset loaded successfully!\")\n",
    "    print(\"\\n--- First 5 rows of Titanic DataFrame ---\")\n",
    "    print(df_titanic.head())\n",
    "    print(\"-\" * 50)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading CSV file: {e}\")\n",
    "    print(\"Please check the URL or your internet connection.\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Iterating through the first 5 rows with df.iterrows() ---\")\n",
    "# Use .head() to limit the iteration for demonstration purposes\n",
    "for index, row in df_titanic.head().iterrows():\n",
    "    print(f\"Index: {index}\")\n",
    "    print(f\"  Name: {row['Name']}\")\n",
    "    print(f\"  Age: {row['Age']}\")\n",
    "    print(f\"  Survived: {row['Survived']}\")\n",
    "    print(f\"  Row (Series):\\n{row.head()}\") # Displaying part of the row Series\n",
    "    print(\"-\" * 20)\n",
    "\n",
    "print(\"\\n--- Example: Using iterrows() to create a new column (for demonstration ONLY, not recommended for large datasets) ---\")\n",
    "\n",
    "# Let's say we want to create a simple 'Age_Group' column\n",
    "# This is a bad way to do it for performance, but good for showing iterrows()\n",
    "age_groups = []\n",
    "for index, row in df_titanic.head(10).iterrows(): # Limiting to 10 rows for example\n",
    "    if pd.isna(row['Age']):\n",
    "        age_groups.append(\"Unknown\")\n",
    "    elif row['Age'] < 18:\n",
    "        age_groups.append(\"Child\")\n",
    "    elif row['Age'] >= 18 and row['Age'] < 60:\n",
    "        age_groups.append(\"Adult\")\n",
    "    else:\n",
    "        age_groups.append(\"Senior\")\n",
    "\n",
    "# Create a new Series from the list and assign it\n",
    "# In a real scenario, you'd assign it to df_titanic['Age_Group'] directly\n",
    "df_temp = df_titanic.head(10).copy() # Create a copy to modify\n",
    "df_temp['Age_Group_Iterrows'] = age_groups\n",
    "print(df_temp[['Name', 'Age', 'Age_Group_Iterrows']])\n",
    "\n",
    "print(\"\\n--- Preferred (Vectorized) Way to Create 'Age_Group' ---\")\n",
    "# This is how you would typically do the above operation efficiently\n",
    "def assign_age_group(age):\n",
    "    if pd.isna(age):\n",
    "        return \"Unknown\"\n",
    "    elif age < 18:\n",
    "        return \"Child\"\n",
    "    elif age >= 18 and age < 60:\n",
    "        return \"Adult\"\n",
    "    else:\n",
    "        return \"Senior\"\n",
    "\n",
    "df_titanic['Age_Group_Vectorized'] = df_titanic['Age'].apply(assign_age_group)\n",
    "print(df_titanic[['Name', 'Age', 'Age_Group_Vectorized']].head(10)) # Display first 10 rows\n",
    "\n",
    "'''\n",
    "When iterrows() might be acceptable:\n",
    "Small DataFrames: If your DataFrame has only a few hundred or a few thousand rows, the performance penalty might be negligible, and the clarity of a simple loop can be fine.\n",
    "Complex Row-wise Logic: When the logic for each row is genuinely complex and doesn't map easily to vectorized operations or apply() (e.g., depending on multiple values from previous rows in a non-trivial way, or involving external API calls per row).\n",
    "Debugging: It can be useful for debugging or inspecting specific rows individually.\n",
    "For most common data manipulation tasks, especially on larger datasets, explore vectorized operations (df['col'] * 2), .apply() (when a Python function needs to be applied element-wise or row-wise), or groupby() before resorting to iterrows().\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71dc62a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# itertuples\n",
    "##############\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "The df.itertuples() method in Pandas is another way to iterate over the rows of a DataFrame, similar to iterrows(), but it is generally much faster and more efficient, especially for large DataFrames.\n",
    "\n",
    "How itertuples() Works\n",
    "itertuples() iterates over the rows of a DataFrame and returns them as named tuples. Each named tuple behaves much like a regular Python tuple but allows you to access elements by attribute name (column name) as well as by numerical position.\n",
    "\n",
    "Advantages of itertuples() over iterrows()\n",
    "Performance: itertuples() is significantly faster than iterrows() because it doesn't create a Pandas Series object for each row, which is a relatively expensive operation.\n",
    "Type Preservation: Named tuples generally preserve the data types of the original DataFrame columns better than the Series objects returned by iterrows().\n",
    "Attribute Access: You can access column values using dot notation (e.g., row.Age) which can make your code more readable than dictionary-style access (row['Age']).\n",
    "Syntax\n",
    "df.itertuples(index=True, name='Pandas')\n",
    "\n",
    "index (bool, default True): If True, the first element of the tuple will be the index of the row. If False, the index is omitted.\n",
    "name (str, default 'Pandas'): The name of the named tuple. You can change this to something more descriptive for your specific data.\n",
    "Example Usage\n",
    "Let's use the Titanic dataset to demonstrate itertuples().\n",
    "\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Titanic dataset\n",
    "titanic_dataset_github_url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/refs/heads/master/titanic.csv\"\n",
    "try:\n",
    "    df_titanic = pd.read_csv(titanic_dataset_github_url)\n",
    "    print(\"Titanic dataset loaded successfully!\")\n",
    "    print(\"\\n--- First 5 rows of Titanic DataFrame ---\")\n",
    "    print(df_titanic.head())\n",
    "    print(\"-\" * 50)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading CSV file: {e}\")\n",
    "    print(\"Please check the URL or your internet connection.\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Iterating through the first 5 rows with df.itertuples() ---\")\n",
    "# Use .head() to limit the iteration for demonstration purposes\n",
    "for row in df_titanic.head().itertuples():\n",
    "    print(f\"Row Tuple: {row}\")\n",
    "    print(f\"  Index: {row.Index}\") # Accessing the index\n",
    "    print(f\"  Name: {row.Name}\")   # Accessing by column name attribute\n",
    "    print(f\"  Age: {row.Age}\")     # Accessing by column name attribute\n",
    "    print(f\"  Survived: {row.Survived}\")\n",
    "    # You can also access by position, but attribute access is often preferred for readability\n",
    "    # print(f\"  PassengerId (pos 1): {row[1]}\")\n",
    "    print(\"-\" * 20)\n",
    "\n",
    "\n",
    "print(\"\\n--- Example: Using itertuples() to calculate a new column (better performance than iterrows()) ---\")\n",
    "\n",
    "# Let's say we want to calculate a simplified 'Fare_Per_Person'\n",
    "# This is still a loop, so vectorized methods are better if possible,\n",
    "# but if you must loop, itertuples is often preferred over iterrows.\n",
    "fare_per_person = []\n",
    "for row in df_titanic.itertuples(index=False): # index=False if you don't need the DataFrame index in the tuple\n",
    "    # Check if 'Fare' and 'SibSp' (Siblings/Spouses) and 'Parch' (Parents/Children) are not NaN\n",
    "    if not pd.isna(row.Fare) and not pd.isna(row.SibSp) and not pd.isna(row.Parch):\n",
    "        # Avoid division by zero if (SibSp + Parch + 1) is 0\n",
    "        num_people_in_group = row.SibSp + row.Parch + 1\n",
    "        if num_people_in_group > 0:\n",
    "            fare_per_person.append(row.Fare / num_people_in_group)\n",
    "        else: # Should not happen if SibSp + Parch is always >= 0 for individuals\n",
    "            fare_per_person.append(0) # Or np.nan\n",
    "    else:\n",
    "        fare_per_person.append(np.nan) # Or 0, depending on how you want to handle missing values\n",
    "\n",
    "# Assign to a new column (this assumes fare_per_person has same length as df_titanic)\n",
    "df_titanic['Fare_Per_Person_Itertuples'] = fare_per_person\n",
    "print(df_titanic[['Name', 'Fare', 'SibSp', 'Parch', 'Fare_Per_Person_Itertuples']].head(10))\n",
    "\n",
    "print(\"\\n--- Preferred (Vectorized) Way to Calculate 'Fare_Per_Person' ---\")\n",
    "# This is how you would typically do the above operation efficiently using vectorized operations\n",
    "df_titanic['Num_People_In_Group'] = df_titanic['SibSp'] + df_titanic['Parch'] + 1\n",
    "df_titanic['Fare_Per_Person_Vectorized'] = df_titanic['Fare'] / df_titanic['Num_People_In_Group']\n",
    "\n",
    "print(df_titanic[['Name', 'Fare', 'SibSp', 'Parch', 'Fare_Per_Person_Vectorized']].head(10))\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "When to Use itertuples():\n",
    "Looping is unavoidable: If your row-wise logic is too complex for vectorized operations or apply() (e.g., depends on state from previous rows, external lookups per row, highly conditional logic across many columns).\n",
    "Performance is crucial: When you need to iterate, and iterrows() is too slow.\n",
    "Attribute access is preferred: When you like accessing columns by row.column_name.\n",
    "Preserving dtypes: When type preservation is important in your loop.\n",
    "Best Practice Reminder: Always try to use vectorized Pandas operations first, as they are almost always the fastest. If you must loop, itertuples() is generally a better choice than iterrows() for performance and type integrity.\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab103bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do we have itercolumns in pandas?\n",
    "\n",
    "'''\n",
    "\n",
    "Pandas does not have a direct df.itercolumns() method in the same way it has df.iterrows() or df.itertuples().\n",
    "\n",
    "However, there are several very straightforward and idiomatic ways to iterate through columns (or access columns) in a Pandas DataFrame:\n",
    "\n",
    "1. Iterating through Column Names (Most Common)\n",
    "The simplest way to iterate through columns is to iterate directly over the DataFrame's columns attribute, which is an Index object containing all column names. You can then use the column name to access the corresponding Series.\n",
    "\n",
    "Python\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "data = {'col1': [1, 2, 3], 'col2': [4, 5, 6], 'col3': [7, 8, 9]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"--- Iterating through column names ---\")\n",
    "for col_name in df.columns:\n",
    "    print(f\"Column Name: {col_name}\")\n",
    "    print(f\"Column Data (Series):\\n{df[col_name].head()}\") # Access the column Series\n",
    "    print(\"-\" * 20)\n",
    "2. Iterating through (Column Name, Series) Pairs (df.items() / df.iteritems())\n",
    "This method is the direct equivalent of df.iterrows() but for columns. It yields (column_name, Series) pairs. df.items() is preferred in modern Pandas (since 1.0.0), while df.iteritems() is deprecated.\n",
    "\n",
    "Python\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "data = {'col1': [1, 2, 3], 'col2': [4, 5, 6], 'col3': [7, 8, 9]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"\\n--- Iterating through (column name, Series) pairs with df.items() ---\")\n",
    "for col_name, col_data in df.items():\n",
    "    print(f\"Column Name: {col_name}\")\n",
    "    print(f\"Column Data (Series):\\n{col_data.head()}\")\n",
    "    print(\"-\" * 20)\n",
    "3. Using .apply() with axis=1 (for row-wise operations) or axis=0 (for column-wise operations)\n",
    "While not direct iteration in the sense of a Python for loop, .apply() is a powerful way to perform operations on columns (or rows).\n",
    "\n",
    "axis=0 (default): Applies a function column-wise (i.e., the function receives each column as a Series).\n",
    "axis=1: Applies a function row-wise (i.e., the function receives each row as a Series).\n",
    "Python\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = {'A': [10, 20, 30], 'B': [1, 2, 3], 'C': [5, 15, 25]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"\\n--- Using .apply(axis=0) to perform column-wise operations ---\")\n",
    "\n",
    "# Example: Calculate the mean of each column\n",
    "column_means = df.apply(np.mean, axis=0)\n",
    "print(\"Mean of each column:\\n\", column_means)\n",
    "\n",
    "# Example: Find the max value in each column\n",
    "column_maxes = df.apply(lambda x: x.max(), axis=0)\n",
    "print(\"\\nMax value in each column:\\n\", column_maxes)\n",
    "Why no itercolumns()?\n",
    "The reason Pandas doesn't have a dedicated itercolumns() method is likely because iterating through column names (df.columns) and then accessing the columns (df[col_name]) is already very explicit, clear, and efficient. Also, df.items() directly provides the (column_name, Series) pairs, covering the common need for such iteration.\n",
    "\n",
    "As with rows, it's generally best practice to use vectorized operations (operations directly on entire Pandas Series or DataFrames) whenever possible for performance, rather than explicit Python loops, even when iterating through columns.\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f0eb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature creation with iterrows - append function and also with apply - lambda functions. \n",
    "\n",
    "'''\n",
    "\n",
    "Let's create a sample dataset and then demonstrate feature creation using \n",
    "both the iterrows() (less efficient but illustrative) and the apply() with lambda (more efficient and idiomatic) methods.\n",
    "\n",
    "'''\n",
    "# 1. Create the Initial Dataset\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame with names and ages\n",
    "data = {\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n",
    "    'Age': [25, 32, 19, 45, 28]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"--- Original DataFrame ---\")\n",
    "print(df)\n",
    "print(\"-\" * 30)\n",
    "\n",
    "\n",
    "# 2. Feature Creation using iterrows() and append() (Less Efficient)\n",
    "\n",
    "'''\n",
    "This method involves explicit looping through rows and is generally discouraged \n",
    "for large datasets due to performance overhead, as it operates row by row in Python.\n",
    "\n",
    "'''\n",
    "\n",
    "# Create an empty list to store the new feature values\n",
    "categories_iterrows = []\n",
    "\n",
    "# Iterate through each row of the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    if row['Age'] > 30:\n",
    "        categories_iterrows.append('Senior')\n",
    "    else:\n",
    "        categories_iterrows.append('Junior')\n",
    "\n",
    "# Add the new 'Category_Iterrows' column to the DataFrame\n",
    "df['Category_Iterrows'] = categories_iterrows\n",
    "\n",
    "print(\"\\n--- DataFrame with 'Category_Iterrows' (using iterrows) ---\")\n",
    "print(df)\n",
    "print(\"-\" * 30)\n",
    "\n",
    "\n",
    "'''\n",
    "Explanation:\n",
    "\n",
    "We create an empty list categories_iterrows.\n",
    "df.iterrows() yields (index, row_Series) pairs.\n",
    "For each row (which is a Pandas Series), we access its 'Age' value.\n",
    "Based on the condition (row['Age'] > 30), we append the appropriate category ('Senior' or 'Junior') to our list.\n",
    "Finally, we assign the populated list to a new column 'Category_Iterrows'. Pandas automatically aligns the list values to the DataFrame's index.\n",
    "3. Feature Creation using lambda and apply() (More Efficient and Idiomatic)\n",
    "This is the preferred Pandas way for element-wise or row-wise transformations. apply() is optimized and often uses C-level implementations under the hood, making it much faster for large datasets.\n",
    "\n",
    "'''\n",
    "\n",
    "# Define a lambda function that takes an age and returns the category\n",
    "# lambda x: ... means x will represent each element of the Series 'df['Age']'\n",
    "classify_age = lambda age: 'Senior' if age > 30 else 'Junior'\n",
    "\n",
    "# Apply the lambda function to the 'Age' column to create the new 'Category_Apply' column\n",
    "df['Category_Apply'] = df['Age'].apply(classify_age)\n",
    "\n",
    "print(\"\\n--- DataFrame with 'Category_Apply' (using lambda and apply) ---\")\n",
    "print(df)\n",
    "print(\"-\" * 30)\n",
    "\n",
    "''''\n",
    "Explanation:\n",
    "\n",
    "We define classify_age as a lambda function. A lambda function is a small, anonymous function. Here, it takes one argument (age) and returns 'Senior' if age is greater than 30, otherwise 'Junior'.\n",
    "df['Age'].apply(classify_age) tells Pandas to take the 'Age' Series, pass each individual value from that Series to our classify_age function, and collect all the returned results into a new Series.\n",
    "This new Series is then directly assigned to the new column 'Category_Apply'.\n",
    "\n",
    "\n",
    "Comparison:\n",
    "iterrows(): More readable for complex, multi-column row-by-row logic, but slow. Avoid for simple transformations.\n",
    "apply() with lambda: Excellent balance of readability and performance for row-wise or element-wise operations. It's generally preferred over iterrows() for feature creation.\n",
    "Vectorized Operations (Even Better!): For very simple numeric operations, direct vectorized operations are the fastest. For this specific 'Senior'/'Junior' classification, you could even do it like this (most efficient):\n",
    "\n",
    "\n",
    "# Most efficient way for this specific logic\n",
    "df['Category_Vectorized'] = 'Junior' # Default all to Junior\n",
    "df.loc[df['Age'] > 30, 'Category_Vectorized'] = 'Senior' # Overwrite where condition is met\n",
    "print(\"\\n--- DataFrame with 'Category_Vectorized' (most efficient) ---\")\n",
    "print(df)\n",
    "In summary, always aim for vectorized solutions first, then apply() with lambda for more complex row-wise logic, and only resort to iterrows() as a last resort or for very small datasets or debugging.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "afbd6ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titanic dataset loaded successfully!\n",
      "\n",
      "--- Original DataFrame Head ---\n",
      "                                                Name   Age     Fare  Pclass\n",
      "0                            Braund, Mr. Owen Harris  22.0   7.2500       3\n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  38.0  71.2833       1\n",
      "2                             Heikkinen, Miss. Laina  26.0   7.9250       3\n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  35.0  53.1000       1\n",
      "4                           Allen, Mr. William Henry  35.0   8.0500       3\n",
      "5                                   Moran, Mr. James   NaN   8.4583       3\n",
      "6                            McCarthy, Mr. Timothy J  54.0  51.8625       1\n",
      "7                     Palsson, Master. Gosta Leonard   2.0  21.0750       3\n",
      "8  Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)  27.0  11.1333       3\n",
      "9                Nasser, Mrs. Nicholas (Adele Achem)  14.0  30.0708       2\n",
      "------------------------------------------------------------\n",
      "\n",
      "--- Sorted by Age (Ascending) ---\n",
      "                                Name   Age      Fare  Pclass\n",
      "803  Thomas, Master. Assad Alexander  0.42    8.5167       3\n",
      "755        Hamalainen, Master. Viljo  0.67   14.5000       2\n",
      "644           Baclini, Miss. Eugenie  0.75   19.2583       3\n",
      "469    Baclini, Miss. Helene Barbara  0.75   19.2583       3\n",
      "78     Caldwell, Master. Alden Gates  0.83   29.0000       2\n",
      "831  Richards, Master. George Sibley  0.83   18.7500       2\n",
      "305   Allison, Master. Hudson Trevor  0.92  151.5500       1\n",
      "386  Goodwin, Master. Sidney Leonard  1.00   46.9000       3\n",
      "172     Johnson, Miss. Eleanor Ileen  1.00   11.1333       3\n",
      "183        Becker, Master. Richard F  1.00   39.0000       2\n",
      "------------------------------------------------------------\n",
      "\n",
      "--- Sorted by Fare (Descending) ---\n",
      "                                                Name   Age      Fare  Pclass\n",
      "679               Cardeza, Mr. Thomas Drake Martinez  36.0  512.3292       1\n",
      "258                                 Ward, Miss. Anna  35.0  512.3292       1\n",
      "737                           Lesurer, Mr. Gustave J  35.0  512.3292       1\n",
      "88                        Fortune, Miss. Mabel Helen  23.0  263.0000       1\n",
      "438                                Fortune, Mr. Mark  64.0  263.0000       1\n",
      "341                   Fortune, Miss. Alice Elizabeth  24.0  263.0000       1\n",
      "27                    Fortune, Mr. Charles Alexander  19.0  263.0000       1\n",
      "742            Ryerson, Miss. Susan Parker \"Suzette\"  21.0  262.3750       1\n",
      "311                       Ryerson, Miss. Emily Borie  18.0  262.3750       1\n",
      "299  Baxter, Mrs. James (Helene DeLaudeniere Chaput)  50.0  247.5208       1\n",
      "------------------------------------------------------------\n",
      "\n",
      "--- Sorted by Pclass (Asc) then Age (Desc) ---\n",
      "                                     Name   Age      Fare  Pclass\n",
      "630  Barkworth, Mr. Algernon Henry Wilson  80.0   30.0000       1\n",
      "96              Goldschmidt, Mr. George B  71.0   34.6542       1\n",
      "493               Artagaveytia, Mr. Ramon  71.0   49.5042       1\n",
      "745          Crosby, Capt. Edward Gifford  70.0   71.0000       1\n",
      "54         Ostby, Mr. Engelhart Cornelius  65.0   61.9792       1\n",
      "456             Millet, Mr. Francis Davis  65.0   26.5500       1\n",
      "438                     Fortune, Mr. Mark  64.0  263.0000       1\n",
      "545          Nicholson, Mr. Arthur Ernest  64.0   26.0000       1\n",
      "275     Andrews, Miss. Kornelia Theodosia  63.0   77.9583       1\n",
      "252             Stead, Mr. William Thomas  62.0   26.5500       1\n",
      "------------------------------------------------------------\n",
      "\n",
      "--- Sorted by Age (NaNs First) ---\n",
      "                                              Name  Age      Fare  Pclass\n",
      "5                                 Moran, Mr. James  NaN    8.4583       3\n",
      "17                    Williams, Mr. Charles Eugene  NaN   13.0000       2\n",
      "19                         Masselmani, Mrs. Fatima  NaN    7.2250       3\n",
      "26                         Emir, Mr. Farred Chehab  NaN    7.2250       3\n",
      "28                   O'Dwyer, Miss. Ellen \"Nellie\"  NaN    7.8792       3\n",
      "29                             Todoroff, Mr. Lalio  NaN    7.8958       3\n",
      "31  Spencer, Mrs. William Augustus (Marie Eugenie)  NaN  146.5208       1\n",
      "32                        Glynn, Miss. Mary Agatha  NaN    7.7500       3\n",
      "36                                Mamee, Mr. Hanna  NaN    7.2292       3\n",
      "42                             Kraeff, Mr. Theodor  NaN    7.8958       3\n",
      "------------------------------------------------------------\n",
      "\n",
      "--- Sorting In-place (df_titanic modified directly) ---\n",
      "df_titanic head BEFORE in-place sort:\n",
      "                                                Name   Age\n",
      "0                            Braund, Mr. Owen Harris  22.0\n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  38.0\n",
      "2                             Heikkinen, Miss. Laina  26.0\n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  35.0\n",
      "4                           Allen, Mr. William Henry  35.0\n",
      "\n",
      "df_titanic head AFTER in-place sort by PassengerId:\n",
      "                                                Name   Age\n",
      "0                            Braund, Mr. Owen Harris  22.0\n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  38.0\n",
      "2                             Heikkinen, Miss. Laina  26.0\n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  35.0\n",
      "4                           Allen, Mr. William Henry  35.0\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# df.sort_values() method\n",
    "\n",
    "\n",
    "'''\n",
    "This is a very common and useful operation for exploring data.\n",
    "\n",
    "df.sort_values(by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last', ignore_index=False, key=None)\n",
    "Key Parameters:\n",
    "\n",
    "by (required): The name or list of names of the columns to sort by. This is the primary parameter.\n",
    "ascending (bool or list of bool, default True): Sort ascending vs. descending.\n",
    "If True, sorts in ascending order.\n",
    "If False, sorts in descending order.\n",
    "If a list of booleans, it must match the length of by, specifying the order for each column.\n",
    "inplace (bool, default False): If True, the operation is performed in place (modifies the original DataFrame) and returns None. If False, it returns a new sorted DataFrame.\n",
    "na_position (str, default 'last'): Controls the position of NaN (missing) values.\n",
    "'first': Puts NaNs at the beginning.\n",
    "'last': Puts NaNs at the end.\n",
    "\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Define the URL for the Titanic dataset\n",
    "titanic_dataset_github_url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/refs/heads/master/titanic.csv\"\n",
    "\n",
    "# Read the CSV file into a Pandas DataFrame\n",
    "try:\n",
    "    df_titanic = pd.read_csv(titanic_dataset_github_url)\n",
    "    print(\"Titanic dataset loaded successfully!\")\n",
    "    print(\"\\n--- Original DataFrame Head ---\")\n",
    "    print(df_titanic[['Name', 'Age', 'Fare', 'Pclass']].head(10)) # Show relevant columns\n",
    "    print(\"-\" * 60)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading CSV file: {e}\")\n",
    "    print(\"Please check the URL or your internet connection.\")\n",
    "\n",
    "# --- 1. Sorting by a Single Column (Age, ascending) ---\n",
    "# Creates a new DataFrame with rows sorted by 'Age'\n",
    "df_sorted_by_age = df_titanic.sort_values(by='Age')\n",
    "\n",
    "print(\"\\n--- Sorted by Age (Ascending) ---\")\n",
    "print(df_sorted_by_age[['Name', 'Age', 'Fare', 'Pclass']].head(10)) # Showing first 10 rows\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# --- 2. Sorting by a Single Column (Fare, descending) ---\n",
    "df_sorted_by_fare_desc = df_titanic.sort_values(by='Fare', ascending=False)\n",
    "\n",
    "print(\"\\n--- Sorted by Fare (Descending) ---\")\n",
    "print(df_sorted_by_fare_desc[['Name', 'Age', 'Fare', 'Pclass']].head(10))\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# --- 3. Sorting by Multiple Columns (Pclass ascending, then Age descending) ---\n",
    "# When sorting by multiple columns, the order matters.\n",
    "# It sorts by the first column, then for rows with identical values in the first column,\n",
    "# it sorts by the second column, and so on.\n",
    "df_sorted_multi = df_titanic.sort_values(by=['Pclass', 'Age'], ascending=[True, False])\n",
    "\n",
    "print(\"\\n--- Sorted by Pclass (Asc) then Age (Desc) ---\")\n",
    "print(df_sorted_multi[['Name', 'Age', 'Fare', 'Pclass']].head(10))\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# --- 4. Handling Missing Values ('na_position') ---\n",
    "# 'Age' column has missing values (NaN). By default, they go to the last.\n",
    "# Let's sort by Age and put NaNs at the beginning.\n",
    "df_sorted_age_nan_first = df_titanic.sort_values(by='Age', na_position='first')\n",
    "\n",
    "print(\"\\n--- Sorted by Age (NaNs First) ---\")\n",
    "print(df_sorted_age_nan_first[['Name', 'Age', 'Fare', 'Pclass']].head(10)) # Will show NaNs at the top\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# --- 5. Sorting In-place (Modifying the original DataFrame) ---\n",
    "# Use with caution, as it permanently changes df_titanic\n",
    "print(\"\\n--- Sorting In-place (df_titanic modified directly) ---\")\n",
    "print(\"df_titanic head BEFORE in-place sort:\")\n",
    "print(df_titanic[['Name', 'Age']].head())\n",
    "\n",
    "df_titanic.sort_values(by='PassengerId', inplace=True) # Sort by original ID to reset order\n",
    "# (or any column that was not sorted before to show modification)\n",
    "\n",
    "print(\"\\ndf_titanic head AFTER in-place sort by PassengerId:\")\n",
    "print(df_titanic[['Name', 'Age']].head())\n",
    "print(\"-\" * 60)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69d770b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titanic dataset loaded successfully!\n",
      "\n",
      "--- 'Age' Column Info Before Filling NaNs ---\n",
      "0    22.0\n",
      "1    38.0\n",
      "2    26.0\n",
      "3    35.0\n",
      "4    35.0\n",
      "5     NaN\n",
      "6    54.0\n",
      "7     2.0\n",
      "8    27.0\n",
      "9    14.0\n",
      "Name: Age, dtype: float64\n",
      "Number of NaN values in 'Age' before fill: 177\n",
      "------------------------------------------------------------\n",
      "\n",
      "--- 'Age' Column Info After Filling NaNs with 0 ---\n",
      "0    22.0\n",
      "1    38.0\n",
      "2    26.0\n",
      "3    35.0\n",
      "4    35.0\n",
      "5     0.0\n",
      "6    54.0\n",
      "7     2.0\n",
      "8    27.0\n",
      "9    14.0\n",
      "Name: Age, dtype: float64\n",
      "Number of NaN values in 'Age' after fill: 0\n",
      "------------------------------------------------------------\n",
      "\n",
      "--- DataFrame Sorted by 'Age' (Ascending, NaNs filled with 0) ---\n",
      "                                  Name  Age     Fare  Pclass\n",
      "28       O'Dwyer, Miss. Ellen \"Nellie\"  0.0   7.8792       3\n",
      "593                 Bourke, Miss. Mary  0.0   7.7500       3\n",
      "457  Kenyon, Mrs. Frederick R (Marion)  0.0  51.8625       1\n",
      "454                Peduzzi, Mr. Joseph  0.0   8.0500       3\n",
      "451    Hagland, Mr. Ingvald Olai Olsen  0.0  19.9667       3\n",
      "589                Murdlin, Mr. Joseph  0.0   8.0500       3\n",
      "584                Paulner, Mr. Uscher  0.0   8.7125       3\n",
      "602          Harrington, Mr. Charles H  0.0  42.4000       1\n",
      "444  Johannesen-Bratthammer, Mr. Bernt  0.0   8.1125       3\n",
      "601               Slabenoff, Mr. Petco  0.0   7.8958       3\n",
      "------------------------------------------------------------\n",
      "\n",
      "--- Some Rows with Age = 0 (previously NaN) ---\n",
      "                                  Name  Age     Fare  Pclass\n",
      "28       O'Dwyer, Miss. Ellen \"Nellie\"  0.0   7.8792       3\n",
      "593                 Bourke, Miss. Mary  0.0   7.7500       3\n",
      "457  Kenyon, Mrs. Frederick R (Marion)  0.0  51.8625       1\n",
      "454                Peduzzi, Mr. Joseph  0.0   8.0500       3\n",
      "451    Hagland, Mr. Ingvald Olai Olsen  0.0  19.9667       3\n",
      "589                Murdlin, Mr. Joseph  0.0   8.0500       3\n",
      "584                Paulner, Mr. Uscher  0.0   8.7125       3\n",
      "602          Harrington, Mr. Charles H  0.0  42.4000       1\n",
      "444  Johannesen-Bratthammer, Mr. Bernt  0.0   8.1125       3\n",
      "601               Slabenoff, Mr. Petco  0.0   7.8958       3\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nExplanation:\\nOriginal 'Age' Column: We first show the head() of the original 'Age' column and count its NaN values using isnull().sum(). You'll see NaN entries.\\ndf_filled_age['Age'].fillna(0): This is the core operation.\\ndf_filled_age['Age'] selects the 'Age' Series.\\n.fillna(0) replaces all NaN values within that Series with the integer 0.\\nWe assign the result back to df_filled_age['Age'] (or use inplace=True) to update the column.\\nSorted DataFrame: After filling, when we sort df_filled_age.sort_values(by='Age'), all the rows where 'Age' was previously NaN (and now 0) will appear at the very top of the sorted DataFrame because 0 is the smallest numerical value.\\nThis demonstrates how fillna() is crucial for preparing data when numerical operations or sorting are affected by missing values.\\n\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Filling NaN values before sorting is a common step in data cleaning and preparation, as NaN values are treated specially by sorting algorithms (typically moved to the beginning or end).\n",
    "\n",
    "Let's fill the NaN values in the 'Age' column of df_titanic with 0 and then sort it.\n",
    "\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np # Often useful for NaN checks\n",
    "\n",
    "# Define the URL for the Titanic dataset\n",
    "titanic_dataset_github_url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/refs/heads/master/titanic.csv\"\n",
    "\n",
    "# Read the CSV file into a Pandas DataFrame\n",
    "try:\n",
    "    df_titanic = pd.read_csv(titanic_dataset_github_url)\n",
    "    print(\"Titanic dataset loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading CSV file: {e}\")\n",
    "    print(\"Please check the URL or your internet connection.\")\n",
    "\n",
    "# --- 1. Inspect 'Age' column before filling NaNs ---\n",
    "print(\"\\n--- 'Age' Column Info Before Filling NaNs ---\")\n",
    "print(df_titanic['Age'].head(10))\n",
    "print(f\"Number of NaN values in 'Age' before fill: {df_titanic['Age'].isnull().sum()}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# --- 2. Fill NaN values in 'Age' with 0 ---\n",
    "# Create a copy to avoid modifying the original df_titanic for further operations\n",
    "df_filled_age = df_titanic.copy()\n",
    "df_filled_age['Age'] = df_filled_age['Age'].fillna(0) # Or df_filled_age['Age'].fillna(0, inplace=True)\n",
    "\n",
    "print(\"\\n--- 'Age' Column Info After Filling NaNs with 0 ---\")\n",
    "print(df_filled_age['Age'].head(10))\n",
    "print(f\"Number of NaN values in 'Age' after fill: {df_filled_age['Age'].isnull().sum()}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# --- 3. Sort the DataFrame by the 'Age' column (ascending) ---\n",
    "# Now, since NaNs are 0, they will appear at the very beginning of the sorted DataFrame.\n",
    "df_sorted_age_filled = df_filled_age.sort_values(by='Age', ascending=True)\n",
    "\n",
    "print(\"\\n--- DataFrame Sorted by 'Age' (Ascending, NaNs filled with 0) ---\")\n",
    "# Displaying the first few rows to show the 0-age entries\n",
    "print(df_sorted_age_filled[['Name', 'Age', 'Fare', 'Pclass']].head(10))\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# --- 4. Verify some rows with 0 age (which were previously NaN) ---\n",
    "print(\"\\n--- Some Rows with Age = 0 (previously NaN) ---\")\n",
    "print(df_sorted_age_filled[df_sorted_age_filled['Age'] == 0][['Name', 'Age', 'Fare', 'Pclass']].head(10))\n",
    "print(\"-\" * 60)\n",
    "\n",
    "'''\n",
    "Explanation:\n",
    "Original 'Age' Column: We first show the head() of the original 'Age' column and count its NaN values using isnull().sum(). You'll see NaN entries.\n",
    "df_filled_age['Age'].fillna(0): This is the core operation.\n",
    "df_filled_age['Age'] selects the 'Age' Series.\n",
    ".fillna(0) replaces all NaN values within that Series with the integer 0.\n",
    "We assign the result back to df_filled_age['Age'] (or use inplace=True) to update the column.\n",
    "Sorted DataFrame: After filling, when we sort df_filled_age.sort_values(by='Age'), all the rows where 'Age' was previously NaN (and now 0) will appear at the very top of the sorted DataFrame because 0 is the smallest numerical value.\n",
    "This demonstrates how fillna() is crucial for preparing data when numerical operations or sorting are affected by missing values.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "45df7c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titanic dataset loaded successfully!\n",
      "\n",
      "--- Original DataFrame Head (Relevant Columns) ---\n",
      "                                                Name  Pclass     Fare   Age\n",
      "0                            Braund, Mr. Owen Harris       3   7.2500  22.0\n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...       1  71.2833  38.0\n",
      "2                             Heikkinen, Miss. Laina       3   7.9250  26.0\n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)       1  53.1000  35.0\n",
      "4                           Allen, Mr. William Henry       3   8.0500  35.0\n",
      "5                                   Moran, Mr. James       3   8.4583   NaN\n",
      "6                            McCarthy, Mr. Timothy J       1  51.8625  54.0\n",
      "7                     Palsson, Master. Gosta Leonard       3  21.0750   2.0\n",
      "8  Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)       3  11.1333  27.0\n",
      "9                Nasser, Mrs. Nicholas (Adele Achem)       2  30.0708  14.0\n",
      "------------------------------------------------------------\n",
      "\n",
      "--- Sorted by Pclass (Ascending) then Fare (Descending) ---\n",
      "                                                  Name  Pclass      Fare   Age\n",
      "258                                   Ward, Miss. Anna       1  512.3292  35.0\n",
      "679                 Cardeza, Mr. Thomas Drake Martinez       1  512.3292  36.0\n",
      "737                             Lesurer, Mr. Gustave J       1  512.3292  35.0\n",
      "27                      Fortune, Mr. Charles Alexander       1  263.0000  19.0\n",
      "88                          Fortune, Miss. Mabel Helen       1  263.0000  23.0\n",
      "341                     Fortune, Miss. Alice Elizabeth       1  263.0000  24.0\n",
      "438                                  Fortune, Mr. Mark       1  263.0000  64.0\n",
      "311                         Ryerson, Miss. Emily Borie       1  262.3750  18.0\n",
      "742              Ryerson, Miss. Susan Parker \"Suzette\"       1  262.3750  21.0\n",
      "118                           Baxter, Mr. Quigg Edmond       1  247.5208  24.0\n",
      "299    Baxter, Mrs. James (Helene DeLaudeniere Chaput)       1  247.5208  50.0\n",
      "380                              Bidois, Miss. Rosalie       1  227.5250  42.0\n",
      "557                                Robbins, Mr. Victor       1  227.5250   NaN\n",
      "700  Astor, Mrs. John Jacob (Madeleine Talmadge Force)       1  227.5250  18.0\n",
      "716                      Endres, Miss. Caroline Louise       1  227.5250  38.0\n",
      "------------------------------------------------------------\n",
      "\n",
      "--- Let's look at a specific Pclass (e.g., Pclass 1) to confirm Fare sorting ---\n",
      "                                      Name  Pclass      Fare   Age\n",
      "258                       Ward, Miss. Anna       1  512.3292  35.0\n",
      "679     Cardeza, Mr. Thomas Drake Martinez       1  512.3292  36.0\n",
      "737                 Lesurer, Mr. Gustave J       1  512.3292  35.0\n",
      "27          Fortune, Mr. Charles Alexander       1  263.0000  19.0\n",
      "88              Fortune, Miss. Mabel Helen       1  263.0000  23.0\n",
      "341         Fortune, Miss. Alice Elizabeth       1  263.0000  24.0\n",
      "438                      Fortune, Mr. Mark       1  263.0000  64.0\n",
      "311             Ryerson, Miss. Emily Borie       1  262.3750  18.0\n",
      "742  Ryerson, Miss. Susan Parker \"Suzette\"       1  262.3750  21.0\n",
      "118               Baxter, Mr. Quigg Edmond       1  247.5208  24.0\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"'\\nExplanation:\\nby=['Pclass', 'Fare']: We provide a list of column names. Pandas will first sort the entire DataFrame by the Pclass column.\\nascending=[True, False]: This list directly corresponds to the by list.\\nTrue for Pclass means it will sort in ascending order (1st class passengers appear before 2nd class, which appear before 3rd class).\\nFalse for Fare means that within each Pclass group, passengers will be sorted by their Fare in descending order (most expensive fares first).\\nAs you can see in the output, the DataFrame is first grouped by Pclass (1, then 2, then 3), and within each of those groups, the Fare values are arranged from highest to lowest. This level of control is incredibly useful for organizing your data exactly how you need it for analysis.\\n\\n\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sorted_values, where by accepts a list of features\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "This is a very common and powerful use case for sort_values() in Pandas. You can absolutely specify a list of columns for by and then a corresponding list of boolean values for ascending to control the sort order for each column individually.\n",
    "\n",
    "Let's use the Titanic dataset to demonstrate this. We'll sort by 'Pclass' in ascending order (1st class first) and then, for passengers within the same 'Pclass', we'll sort by 'Fare' in descending order (most expensive fare first within that class).\n",
    "\n",
    "Sorting by Multiple Columns with Mixed Order\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Define the URL for the Titanic dataset\n",
    "titanic_dataset_github_url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/refs/heads/master/titanic.csv\"\n",
    "\n",
    "# Read the CSV file into a Pandas DataFrame\n",
    "try:\n",
    "    df_titanic = pd.read_csv(titanic_dataset_github_url)\n",
    "    print(\"Titanic dataset loaded successfully!\")\n",
    "    print(\"\\n--- Original DataFrame Head (Relevant Columns) ---\")\n",
    "    print(df_titanic[['Name', 'Pclass', 'Fare', 'Age']].head(10)) # Show relevant columns\n",
    "    print(\"-\" * 60)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading CSV file: {e}\")\n",
    "    print(\"Please check the URL or your internet connection.\")\n",
    "\n",
    "# Sort by 'Pclass' in ascending order, then by 'Fare' in descending order\n",
    "# Notice the list of column names for 'by' and the corresponding list of booleans for 'ascending'\n",
    "df_sorted_mixed = df_titanic.sort_values(by=['Pclass', 'Fare'], ascending=[True, False])\n",
    "\n",
    "print(\"\\n--- Sorted by Pclass (Ascending) then Fare (Descending) ---\")\n",
    "print(df_sorted_mixed[['Name', 'Pclass', 'Fare', 'Age']].head(15)) # Display more rows to see the effect\n",
    "print(\"-\" * 60)\n",
    "\n",
    "print(\"\\n--- Let's look at a specific Pclass (e.g., Pclass 1) to confirm Fare sorting ---\")\n",
    "# Filter for Pclass 1 and check the Fare column to verify descending order\n",
    "pclass_1_sorted = df_sorted_mixed[df_sorted_mixed['Pclass'] == 1]\n",
    "print(pclass_1_sorted[['Name', 'Pclass', 'Fare', 'Age']].head(10))\n",
    "print(\"-\" * 60)\n",
    "\n",
    "\n",
    "''''\n",
    "Explanation:\n",
    "by=['Pclass', 'Fare']: We provide a list of column names. Pandas will first sort the entire DataFrame by the Pclass column.\n",
    "ascending=[True, False]: This list directly corresponds to the by list.\n",
    "True for Pclass means it will sort in ascending order (1st class passengers appear before 2nd class, which appear before 3rd class).\n",
    "False for Fare means that within each Pclass group, passengers will be sorted by their Fare in descending order (most expensive fares first).\n",
    "As you can see in the output, the DataFrame is first grouped by Pclass (1, then 2, then 3), and within each of those groups, the Fare values are arranged from highest to lowest. This level of control is incredibly useful for organizing your data exactly how you need it for analysis.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af17ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#df.sort_index()\n",
    "########################\n",
    "\n",
    "'''df.sort_index(), is a powerful Pandas method used to sort a DataFrame or Series by its index labels (row labels) or its column labels.\n",
    "\n",
    "Unlike sort_values(), which sorts based on the data within columns, sort_index() specifically operates on the labels of the axes.\n",
    "\n",
    "df.sort_index(axis=0, level=None, ascending=True, inplace=False, kind='quicksort', na_position='last', sort_remaining=True, ignore_index=False, key=None)\n",
    "Key Parameters:\n",
    "\n",
    "axis (int or str, default 0):\n",
    "0 or 'index': Sorts by the row labels (index).\n",
    "1 or 'columns': Sorts by the column labels (column names).\n",
    "level (int or label or list, default None): For MultiIndex (hierarchical index), specifies which level(s) to sort by. If None, sorts by all levels.\n",
    "ascending (bool or list of bool, default True): Sort ascending or descending.\n",
    "inplace (bool, default False): If True, modifies the original DataFrame and returns None.\n",
    "na_position (str, default 'last'): How to handle NaN values in the index (less common but possible).\n",
    "Let's demonstrate with the Titanic dataset. First, we'll set the 'Name' column as the index so we have meaningful row labels to sort.\n",
    "\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Define the URL for the Titanic dataset\n",
    "titanic_dataset_github_url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/refs/heads/master/titanic.csv\"\n",
    "\n",
    "# Read the CSV file into a Pandas DataFrame\n",
    "try:\n",
    "    df_titanic = pd.read_csv(titanic_dataset_github_url)\n",
    "    print(\"Titanic dataset loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading CSV file: {e}\")\n",
    "    print(\"Please check the URL or your internet connection.\")\n",
    "\n",
    "# Set 'Name' as the index for better demonstration of row label sorting\n",
    "df_by_name = df_titanic.set_index('Name')\n",
    "\n",
    "print(\"--- Original df_by_name Head (Index not sorted alphabetically) ---\")\n",
    "print(df_by_name[['Age', 'Sex', 'Fare', 'Pclass']].head())\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# --- 1. Sorting by Row Index (default behavior) ---\n",
    "# Sorts the DataFrame rows based on the alphabetical order of the 'Name' index\n",
    "df_sorted_by_index_asc = df_by_name.sort_index(ascending=True)\n",
    "\n",
    "print(\"\\n--- Sorted by Row Index (Name), Ascending ---\")\n",
    "print(df_sorted_by_index_asc[['Age', 'Sex', 'Fare', 'Pclass']].head())\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# --- 2. Sorting by Row Index (descending) ---\n",
    "df_sorted_by_index_desc = df_by_name.sort_index(ascending=False)\n",
    "\n",
    "print(\"\\n--- Sorted by Row Index (Name), Descending ---\")\n",
    "print(df_sorted_by_index_desc[['Age', 'Sex', 'Fare', 'Pclass']].head())\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# --- 3. Sorting by Column Index (Column Names) ---\n",
    "# Creates a new DataFrame for this specific demo, with unsorted columns initially\n",
    "df_unsorted_cols = df_titanic[['Name', 'Fare', 'Age', 'Pclass', 'Sex']].copy()\n",
    "\n",
    "print(\"\\n--- DataFrame with Unsorted Columns ---\")\n",
    "print(df_unsorted_cols.head())\n",
    "print(\"Original column order:\", df_unsorted_cols.columns.tolist())\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Sort the DataFrame columns alphabetically by their names\n",
    "df_sorted_cols = df_unsorted_cols.sort_index(axis=1) # axis=1 specifies sorting columns\n",
    "\n",
    "print(\"\\n--- DataFrame Sorted by Column Index (Column Names), Ascending ---\")\n",
    "print(df_sorted_cols.head())\n",
    "print(\"New column order:\", df_sorted_cols.columns.tolist())\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# --- 4. Sorting by Column Index (Descending) ---\n",
    "df_sorted_cols_desc = df_unsorted_cols.sort_index(axis=1, ascending=False)\n",
    "\n",
    "print(\"\\n--- DataFrame Sorted by Column Index (Column Names), Descending ---\")\n",
    "print(df_sorted_cols_desc.head())\n",
    "print(\"New column order:\", df_sorted_cols_desc.columns.tolist())\n",
    "print(\"-\" * 60)\n",
    "\n",
    "'''\n",
    "When to use sort_index():\n",
    "Organizing by Identifier: When your DataFrame's index is a meaningful identifier \n",
    "(like a unique ID, name, or date), sort_index() helps you arrange your data by that identifier.\n",
    "Consistency: To ensure that DataFrames or Series are consistently ordered by their labels \n",
    "before performing operations like concatenation or merging.\n",
    "\n",
    "Preparing for MultiIndex Operations: Crucial for efficient operations on MultiIndex \n",
    "DataFrames (e.g., groupby operations often perform better on sorted MultiIndexes).\n",
    "\n",
    "Standardizing Column Order: To arrange your DataFrame's columns alphabetically or\n",
    " in a specific desired order for presentation or further processing.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5289ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort_index() with axis=1:  (sorting df by column names)\n",
    "\n",
    "'''\n",
    "\n",
    "When you use sort_index() with axis=1, you are telling Pandas to sort the DataFrame's columns themselves based on their labels (the column names), rather than sorting the rows based on column values.\n",
    "\n",
    "Here's a clear example:\n",
    "\n",
    "Python\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame with columns in an arbitrary order\n",
    "data = {\n",
    "    'Z_Data': [10, 20, 30],\n",
    "    'A_Value': [1, 2, 3],\n",
    "    'C_Item': [100, 200, 300],\n",
    "    'B_Name': ['Alice', 'Bob', 'Charlie']\n",
    "}\n",
    "df_unsorted_cols = pd.DataFrame(data)\n",
    "\n",
    "print(\"--- Original DataFrame (Columns Not Sorted) ---\")\n",
    "print(df_unsorted_cols)\n",
    "print(\"\\nOriginal Column Order:\", df_unsorted_cols.columns.tolist())\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# --- 1. Sort Columns by their Names in Ascending Order ---\n",
    "# axis=1 tells sort_index to operate on the columns\n",
    "df_sorted_cols_asc = df_unsorted_cols.sort_index(axis=1, ascending=True)\n",
    "\n",
    "print(\"\\n--- DataFrame Sorted by Column Names (Ascending) ---\")\n",
    "print(df_sorted_cols_asc)\n",
    "print(\"\\nNew Column Order (Ascending):\", df_sorted_cols_asc.columns.tolist())\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# --- 2. Sort Columns by their Names in Descending Order ---\n",
    "df_sorted_cols_desc = df_unsorted_cols.sort_index(axis=1, ascending=False)\n",
    "\n",
    "print(\"\\n--- DataFrame Sorted by Column Names (Descending) ---\")\n",
    "print(df_sorted_cols_desc)\n",
    "print(\"\\nNew Column Order (Descending):\", df_sorted_cols_desc.columns.tolist())\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# --- Contrast with sort_values(axis=1) for clarity ---\n",
    "print(\"\\n--- IMPORTANT CONTRAST: sort_values(axis=1) ---\")\n",
    "print(\"sort_values(axis=1) sorts the *values within each row*.\")\n",
    "print(\"It does NOT reorder the columns, but rather reorders the values PER ROW.\")\n",
    "\n",
    "df_values_sorted_row_wise = df_unsorted_cols[['A_Value', 'C_Item', 'Z_Data']].apply(lambda row: sorted(row), axis=1, result_type='expand')\n",
    "# Note: apply(sorted, axis=1) would return a Series of lists.\n",
    "# Using a slightly more complex apply with expand to create a new DataFrame.\n",
    "# For simple cases like numerical columns, sort_values(axis=1) can sometimes imply reordering values.\n",
    "# However, for mixed types or general use, it's typically about sorting the *values* for comparison or specific operations.\n",
    "\n",
    "# A common use case for sort_values(axis=1) is finding min/max across rows:\n",
    "df_min_max_row_wise = df_unsorted_cols[['A_Value', 'C_Item', 'Z_Data']].copy()\n",
    "df_min_max_row_wise['Min_Value_in_Row'] = df_min_max_row_wise.min(axis=1)\n",
    "df_min_max_row_wise['Max_Value_in_Row'] = df_min_max_row_wise.max(axis=1)\n",
    "print(\"\\nDataFrame with Min/Max values per row (using min/max with axis=1):\")\n",
    "print(df_min_max_row_wise)\n",
    "print(\"-\" * 50)\n",
    "Key Differences:\n",
    "df.sort_index(axis=1): This method reorders the actual columns of your DataFrame based on their names (labels). It changes the column order in the DataFrame itself.\n",
    "df.sort_values(axis=1): This method sorts the values within each row across the specified columns. It does not change the order of the columns in the DataFrame's structure, but it can be used to reorder the values within a row or to find row-wise statistics (like min/max).\n",
    "So, when you want to arrange your columns alphabetically or in a specific order by their names, df.sort_index(axis=1) is the method you need.\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a93017a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Original DataFrame ---\n",
      "   Z_Score   A_Name  C_Value B_Type\n",
      "0       10    Alice      100      X\n",
      "1       20      Bob      200      Y\n",
      "2       30  Charlie      300      Z\n",
      "\n",
      "Original DataFrame Column Order (df.columns): ['Z_Score', 'A_Name', 'C_Value', 'B_Type']\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Result of sorted(df.columns) ---\n",
      "Type of result: <class 'list'>\n",
      "Sorted column names (Python list): ['A_Name', 'B_Type', 'C_Value', 'Z_Score']\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Original DataFrame (still unchanged) ---\n",
      "   Z_Score   A_Name  C_Value B_Type\n",
      "0       10    Alice      100      X\n",
      "1       20      Bob      200      Y\n",
      "2       30  Charlie      300      Z\n",
      "Original DataFrame Column Order (df.columns): ['Z_Score', 'A_Name', 'C_Value', 'B_Type']\n",
      "--------------------------------------------------\n",
      "\n",
      "--- DataFrame with Columns Reordered using sorted(df.columns) ---\n",
      "    A_Name B_Type  C_Value  Z_Score\n",
      "0    Alice      X      100       10\n",
      "1      Bob      Y      200       20\n",
      "2  Charlie      Z      300       30\n",
      "New DataFrame Column Order: ['A_Name', 'B_Type', 'C_Value', 'Z_Score']\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nContrast with df.sort_index(axis=1)\\nsorted(df.columns): Returns a Python list of sorted column names. It does not alter the DataFrame. You then use this list to create a new DataFrame with the desired column order, like df[sorted_column_names].\\ndf.sort_index(axis=1): This is a Pandas DataFrame method that returns a new Pandas DataFrame (or modifies in place if inplace=True) with its columns sorted by their labels. It directly operates on and modifies the DataFrame's structure.\\nChoose sorted(df.columns) when you just need the column names as a sorted list for, say, printing, looping, or for manually reindexing later. Choose df.sort_index(axis=1) when you want the DataFrame itself to have its columns structurally sorted by their names.\\n\\n\\n\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sorted(df.columns)\n",
    "\n",
    "###########################\n",
    "\n",
    "\n",
    "'''\n",
    "common Pythonic way to get a sorted list of your DataFrame's column names: sorted(df.columns).\n",
    "\n",
    "Here's what it does and how it differs from df.sort_index(axis=1):\n",
    "\n",
    "What sorted(df.columns) Does\n",
    "df.columns returns a Pandas Index object containing all the column labels of your DataFrame.\n",
    "sorted() is a built-in Python function that takes an iterable (like df.columns) and returns a new Python list containing all items from the iterable in sorted order.\n",
    "Key point: sorted(df.columns) produces a Python list of sorted column names; it does NOT modify the order of columns in your DataFrame.\n",
    "\n",
    "Example\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame with columns in an arbitrary order\n",
    "data = {\n",
    "    'Z_Score': [10, 20, 30],\n",
    "    'A_Name': ['Alice', 'Bob', 'Charlie'],\n",
    "    'C_Value': [100, 200, 300],\n",
    "    'B_Type': ['X', 'Y', 'Z']\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"--- Original DataFrame ---\")\n",
    "print(df)\n",
    "print(\"\\nOriginal DataFrame Column Order (df.columns):\", df.columns.tolist())\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Get a sorted list of column names\n",
    "sorted_column_names = sorted(df.columns)\n",
    "\n",
    "print(\"\\n--- Result of sorted(df.columns) ---\")\n",
    "print(\"Type of result:\", type(sorted_column_names))\n",
    "print(\"Sorted column names (Python list):\", sorted_column_names)\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Verify that the original DataFrame's column order is unchanged\n",
    "print(\"\\n--- Original DataFrame (still unchanged) ---\")\n",
    "print(df)\n",
    "print(\"Original DataFrame Column Order (df.columns):\", df.columns.tolist())\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# How to use this list to reorder the DataFrame's columns (if desired)\n",
    "df_reordered_cols = df[sorted_column_names]\n",
    "print(\"\\n--- DataFrame with Columns Reordered using sorted(df.columns) ---\")\n",
    "print(df_reordered_cols)\n",
    "print(\"New DataFrame Column Order:\", df_reordered_cols.columns.tolist())\n",
    "print(\"-\" * 50)\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Contrast with df.sort_index(axis=1)\n",
    "sorted(df.columns): Returns a Python list of sorted column names. It does not alter the DataFrame. You then use this list to create a new DataFrame with the desired column order, like df[sorted_column_names].\n",
    "df.sort_index(axis=1): This is a Pandas DataFrame method that returns a new Pandas DataFrame (or modifies in place if inplace=True) with its columns sorted by their labels. It directly operates on and modifies the DataFrame's structure.\n",
    "Choose sorted(df.columns) when you just need the column names as a sorted list for, say, printing, looping, or for manually reindexing later. Choose df.sort_index(axis=1) when you want the DataFrame itself to have its columns structurally sorted by their names.\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0dcacf4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titanic dataset loaded successfully!\n",
      "\n",
      "--- Original 'Name' Column Head ---\n",
      "0                              Braund, Mr. Owen Harris\n",
      "1    Cumings, Mrs. John Bradley (Florence Briggs Th...\n",
      "2                               Heikkinen, Miss. Laina\n",
      "3         Futrelle, Mrs. Jacques Heath (Lily May Peel)\n",
      "4                             Allen, Mr. William Henry\n",
      "Name: Name, dtype: object\n",
      "------------------------------------------------------------\n",
      "\n",
      "--- 'Name' Column with Length (using .str.len()) ---\n",
      "                                                Name  Name_Length\n",
      "0                            Braund, Mr. Owen Harris           23\n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...           51\n",
      "2                             Heikkinen, Miss. Laina           22\n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)           44\n",
      "4                           Allen, Mr. William Henry           24\n",
      "\n",
      "Average name length: 26.97\n",
      "------------------------------------------------------------\n",
      "\n",
      "--- 'Name' Column Capitalized (using .str.capitalize()) ---\n",
      "                                                Name  \\\n",
      "0                            Braund, Mr. Owen Harris   \n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...   \n",
      "2                             Heikkinen, Miss. Laina   \n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)   \n",
      "4                           Allen, Mr. William Henry   \n",
      "\n",
      "                                    Name_Capitalized  \n",
      "0                            Braund, mr. owen harris  \n",
      "1  Cumings, mrs. john bradley (florence briggs th...  \n",
      "2                             Heikkinen, miss. laina  \n",
      "3       Futrelle, mrs. jacques heath (lily may peel)  \n",
      "4                           Allen, mr. william henry  \n",
      "------------------------------------------------------------\n",
      "\n",
      "--- Examples of .str.capitalize() vs .str.title() ---\n",
      "Original:\n",
      "0          john doe\n",
      "1         mr. smith\n",
      "2    dr. sarah jane\n",
      "dtype: object\n",
      "\n",
      ".str.capitalize():\n",
      "0          John doe\n",
      "1         Mr. smith\n",
      "2    Dr. sarah jane\n",
      "dtype: object\n",
      "\n",
      ".str.title():\n",
      "0          John Doe\n",
      "1         Mr. Smith\n",
      "2    Dr. Sarah Jane\n",
      "dtype: object\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nExplanation:\\n.str Accessor: When you see Series.str, it means you\\'re accessing a set of string methods that can be applied to each element of the Series. These methods are vectorized, meaning they are very efficient for large datasets.\\n.str.len(): Calculates the length of each string. For a name like \"Braund, Mr. Owen Harris\", it counts all characters including spaces and punctuation.\\n.str.capitalize(): Converts the first character of each string to uppercase and all remaining characters to lowercase. This is important to note: it doesn\\'t capitalize the first letter of each word, only the very first letter of the entire string. If you wanted to capitalize the first letter of each word (like in titles), you\\'d typically use .str.title().\\n\\n\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# str.capitalize() and str.len()\n",
    "###############################\n",
    "\n",
    "''' \n",
    "\n",
    "Pandas offers powerful string methods via the .str accessor, which allows you to apply string operations to entire Series (columns) containing text data.\n",
    "\n",
    "Let's use df_titanic['Name'].str.len() to get the length of each name and df_titanic['Name'].str.capitalize() to capitalize the first letter of each name.\n",
    "\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Define the URL for the Titanic dataset\n",
    "titanic_dataset_github_url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/refs/heads/master/titanic.csv\"\n",
    "\n",
    "# Read the CSV file into a Pandas DataFrame\n",
    "try:\n",
    "    df_titanic = pd.read_csv(titanic_dataset_github_url)\n",
    "    print(\"Titanic dataset loaded successfully!\")\n",
    "    print(\"\\n--- Original 'Name' Column Head ---\")\n",
    "    print(df_titanic['Name'].head())\n",
    "    print(\"-\" * 60)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading CSV file: {e}\")\n",
    "    print(\"Please check the URL or your internet connection.\")\n",
    "\n",
    "# --- 1. Use .str.len() to get the length of each name ---\n",
    "# This will return a Series containing the length of each string in the 'Name' column.\n",
    "name_lengths = df_titanic['Name'].str.len()\n",
    "\n",
    "# Add this as a new column to the DataFrame for display\n",
    "df_titanic['Name_Length'] = name_lengths\n",
    "\n",
    "print(\"\\n--- 'Name' Column with Length (using .str.len()) ---\")\n",
    "print(df_titanic[['Name', 'Name_Length']].head())\n",
    "print(f\"\\nAverage name length: {df_titanic['Name_Length'].mean():.2f}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "\n",
    "# --- 2. Use .str.capitalize() to capitalize the first letter of each name ---\n",
    "# This returns a Series where the first character of each string is capitalized,\n",
    "# and the rest are lowercased.\n",
    "capitalized_names = df_titanic['Name'].str.capitalize()\n",
    "\n",
    "# Add this as a new column (or overwrite for demonstration)\n",
    "df_titanic['Name_Capitalized'] = capitalized_names\n",
    "\n",
    "print(\"\\n--- 'Name' Column Capitalized (using .str.capitalize()) ---\")\n",
    "print(df_titanic[['Name', 'Name_Capitalized']].head())\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# --- Displaying a few names to show the effect of capitalize ---\n",
    "# Note: capitalize only capitalizes the *first* letter of the entire string.\n",
    "# It doesn't handle individual words like .str.title() would.\n",
    "print(\"\\n--- Examples of .str.capitalize() vs .str.title() ---\")\n",
    "sample_names = pd.Series([\"john doe\", \"mr. smith\", \"dr. sarah jane\"])\n",
    "print(f\"Original:\\n{sample_names}\")\n",
    "print(f\"\\n.str.capitalize():\\n{sample_names.str.capitalize()}\")\n",
    "print(f\"\\n.str.title():\\n{sample_names.str.title()}\") # Capitalizes first letter of each word\n",
    "print(\"-\" * 60)\n",
    "\n",
    "'''\n",
    "Explanation:\n",
    ".str Accessor: When you see Series.str, it means you're accessing a set of string methods that can be applied to each element of the Series. These methods are vectorized, meaning they are very efficient for large datasets.\n",
    ".str.len(): Calculates the length of each string. For a name like \"Braund, Mr. Owen Harris\", it counts all characters including spaces and punctuation.\n",
    ".str.capitalize(): Converts the first character of each string to uppercase and all remaining characters to lowercase. This is important to note: it doesn't capitalize the first letter of each word, only the very first letter of the entire string. If you wanted to capitalize the first letter of each word (like in titles), you'd typically use .str.title().\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da96a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex\n",
    "################\n",
    "\n",
    "'''\n",
    "\n",
    "Regular Expressions (often shortened to \"regex\" or \"regexp\") are sequences of characters that define a search pattern.\n",
    "They are incredibly powerful for finding, matching, and manipulating text based on complex rules.\n",
    "Think of them as a highly advanced \"find and replace\" tool.\n",
    "\n",
    "Basic Regex Components\n",
    "Literals: Most characters match themselves (e.g., a matches 'a').\n",
    "Metacharacters: Special characters with specific meanings:\n",
    ".: Matches any single character (except newline).\n",
    "*: Matches 0 or more occurrences of the preceding character/group.\n",
    "+: Matches 1 or more occurrences of the preceding character/group.\n",
    "?: Matches 0 or 1 occurrence of the preceding character/group.\n",
    "[]: Matches any one of the characters inside the brackets (e.g., [abc] matches 'a', 'b', or 'c').\n",
    "[^]: Matches any character not inside the brackets (e.g., [^0-9] matches any non-digit).\n",
    "(): Used for grouping and capturing substrings.\n",
    "|: OR operator (e.g., cat|dog matches 'cat' or 'dog').\n",
    "\\: Escape character (e.g., \\. matches a literal dot, \\d matches a digit).\n",
    "Anchors:\n",
    "^: Matches the beginning of the string.\n",
    "$: Matches the end of the string.\n",
    "Quantifiers:\n",
    "{n}: Matches exactly n occurrences.\n",
    "{n,}: Matches n or more occurrences.\n",
    "{n,m}: Matches between n and m occurrences.\n",
    "Simple Example: Email ID Validation (Python's re module)\n",
    "Let's create a very basic (and not exhaustive for real-world scenarios) regex for an email address.\n",
    "\n",
    "Regex Pattern: ^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\n",
    "\n",
    "^: Start of the string.\n",
    "[a-zA-Z0-9._%+-]+: One or more letters, numbers, dots, underscores, percents, plus, or hyphens (the \"username\" part).\n",
    "@: Matches the literal '@' symbol.\n",
    "[a-zA-Z0-9.-]+: One or more letters, numbers, dots, or hyphens (the \"domain name\" part).\n",
    "\\.: Matches a literal dot (escaped because . is a metacharacter).\n",
    "[a-zA-Z]{2,}: Two or more letters (the \"top-level domain\" like .com, .org, .net).\n",
    "$: End of the string.\n",
    "Python\n",
    "\n",
    "import re\n",
    "\n",
    "email_pattern = r\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\"\n",
    "# The 'r' before the string means it's a \"raw string\", which prevents backslashes\n",
    "# from being interpreted as escape sequences by Python itself. This is best practice for regex.\n",
    "\n",
    "email1 = \"test.user@example.com\"\n",
    "email2 = \"invalid-email\"\n",
    "email3 = \"another.one@sub.domain.co.uk\"\n",
    "\n",
    "print(\"--- Email ID Validation Example ---\")\n",
    "\n",
    "# re.match() checks for a match only at the beginning of the string\n",
    "if re.match(email_pattern, email1):\n",
    "    print(f\"'{email1}' is a VALID email (using re.match)\")\n",
    "else:\n",
    "    print(f\"'{email1}' is an INVALID email (using re.match)\")\n",
    "\n",
    "if re.match(email_pattern, email2):\n",
    "    print(f\"'{email2}' is a VALID email (using re.match)\")\n",
    "else:\n",
    "    print(f\"'{email2}' is an INVALID email (using re.match)\")\n",
    "\n",
    "print(\"\\n(Using re.search to find patterns anywhere in the string)\")\n",
    "# re.search() finds a match anywhere in the string\n",
    "email_in_text = \"My email is test@domain.org, please contact me.\"\n",
    "match = re.search(r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\", email_in_text)\n",
    "if match:\n",
    "    print(f\"Found email: '{match.group(0)}' in the text.\")\n",
    "else:\n",
    "    print(\"No email found in the text.\")\n",
    "print(\"-\" * 50)\n",
    "Example Using Titanic Dataset: Extracting Titles\n",
    "In the Titanic dataset, the 'Name' column often contains titles like \"Mr.\", \"Mrs.\", \"Miss.\", \"Dr.\", \"Rev.\", \"Master.\", etc. We can use regex to extract these titles.\n",
    "\n",
    "Regex Pattern: ([A-Za-z]+)\\.\n",
    "\n",
    "( ): This creates a capturing group. Whatever matches inside these parentheses will be extracted.\n",
    "[A-Za-z]+: Matches one or more uppercase or lowercase English letters.\n",
    "\\.: Matches a literal dot (the period after the title).\n",
    "Python\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Titanic dataset\n",
    "titanic_dataset_github_url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/refs/heads/master/titanic.csv\"\n",
    "try:\n",
    "    df_titanic = pd.read_csv(titanic_dataset_github_url)\n",
    "    print(\"Titanic dataset loaded successfully!\")\n",
    "    print(\"\\n--- Original 'Name' Column Head ---\")\n",
    "    print(df_titanic['Name'].head(10))\n",
    "    print(\"-\" * 60)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading CSV file: {e}\")\n",
    "    print(\"Please check the URL or your internet connection.\")\n",
    "\n",
    "# Regex to extract titles from the Name column\n",
    "title_pattern = r'([A-Za-z]+)\\.' # Captures one or more letters followed by a dot\n",
    "\n",
    "# Using .str.extract() to extract the captured group\n",
    "# .str.extract() returns a DataFrame with one column for each capturing group.\n",
    "df_titanic['Title'] = df_titanic['Name'].str.extract(title_pattern, expand=False)\n",
    "# expand=False makes it return a Series if there's only one capturing group.\n",
    "\n",
    "print(\"\\n--- DataFrame with Extracted Titles using .str.extract() ---\")\n",
    "print(df_titanic[['Name', 'Title']].head(10))\n",
    "print(\"-\" * 60)\n",
    "\n",
    "print(\"\\n--- Value Counts of Extracted Titles ---\")\n",
    "print(df_titanic['Title'].value_counts())\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Another common .str method for regex: .str.contains()\n",
    "# Let's find all names containing 'Mrs.'\n",
    "mrs_names = df_titanic[df_titanic['Name'].str.contains(r'Mrs\\.', na=False)]\n",
    "print(\"\\n--- Passengers with 'Mrs.' in their Name (using .str.contains()) ---\")\n",
    "print(mrs_names[['Name', 'Title', 'Sex', 'Age']].head())\n",
    "print(f\"\\nTotal passengers with 'Mrs.' in name: {len(mrs_names)}\")\n",
    "print(\"-\" * 60)\n",
    "Key Points for Regex in Pandas:\n",
    ".str Accessor: All string methods in Pandas that work with regex patterns are accessed through the .str accessor (e.g., Series.str.contains(), Series.str.extract(), Series.str.replace(), Series.str.match()).\n",
    "regex=True (or implied): For many .str methods, regex=True is the default (or implied) when the method supports regex (e.g., contains, match, extract, replace). You typically don't need to explicitly set it.\n",
    "Raw Strings (r\"...\"): Always use raw strings (r\"your_pattern\") for regex patterns in Python. This prevents backslashes from being interpreted as Python escape sequences, which is crucial for regex (e.g., \\d means digit in regex, but \\d would be an invalid escape sequence in a normal string).\n",
    "Regex is a vast topic, but understanding these basics allows you to perform powerful text manipulation in Pandas and Python.\n",
    "\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6b6712",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Extract parts of the name from the Titanic dataset's 'Name' column using regular expressions, \n",
    "specifically demonstrating the effect of expand=False versus expand=True on the output's structure and dtype.\n",
    "\n",
    "The 'Name' column in the Titanic dataset is typically formatted as \"Surname, Title. GivenName MiddleName\". \n",
    "We'll write a regex to extract the Surname and the First Name (the first given name after the title).\n",
    "\n",
    "Regex Pattern: r'([^,]+),\\s*[A-Za-z]+\\.\\s*([A-Za-z]+)'\n",
    "\n",
    "Let's break down this regex:\n",
    "\n",
    "( ): These create capturing groups. Whatever matches inside them will be extracted into the output.\n",
    "([^,]+):\n",
    "[^,]: Matches any character that is not a comma.\n",
    "+: Matches the preceding character set one or more times.\n",
    "This first capturing group ([^,]+) will capture the Surname (e.g., \"Braund\", \"Cumings\").\n",
    ",\\s*: Matches a literal comma , followed by zero or more whitespace characters (\\s*). This part is matched but not captured.\n",
    "[A-Za-z]+\\.\\s*: Matches the Title part (e.g., \"Mr.\", \"Mrs.\", \"Miss.\").\n",
    "[A-Za-z]+: One or more letters (for \"Mr\", \"Mrs\", etc.).\n",
    "\\.: A literal dot (escaped with \\).\n",
    "\\s*: Zero or more whitespace characters after the title. This part is matched but not captured.\n",
    "([A-Za-z]+):\n",
    "[A-Za-z]+: Matches one or more letters.\n",
    "This second capturing group ([A-Za-z]+) will capture the First Name (e.g., \"Owen\", \"Elizabeth\").\n",
    "Now, let's see expand=False vs expand=True:\n",
    "\n",
    "Python\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Define the URL for the Titanic dataset\n",
    "titanic_dataset_github_url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/refs/heads/master/titanic.csv\"\n",
    "\n",
    "# Read the CSV file into a Pandas DataFrame\n",
    "try:\n",
    "    df_titanic = pd.read_csv(titanic_dataset_github_url)\n",
    "    print(\"Titanic dataset loaded successfully!\")\n",
    "    print(\"\\n--- Original 'Name' Column Head (for reference) ---\")\n",
    "    print(df_titanic['Name'].head(7))\n",
    "    print(\"-\" * 70)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading CSV file: {e}\")\n",
    "    print(\"Please check the URL or your internet connection.\")\n",
    "\n",
    "# Define the regex pattern\n",
    "name_pattern = r'([^,]+),\\s*[A-Za-z]+\\.\\s*([A-Za-z]+)'\n",
    "\n",
    "# --- 1. Using expand=False ---\n",
    "# When expand=False and there are multiple capturing groups,\n",
    "# it returns a Series where each element is a tuple of the captured groups.\n",
    "extracted_names_false = df_titanic['Name'].str.extract(name_pattern, expand=False)\n",
    "\n",
    "print(\"\\n--- Result with expand=False ---\")\n",
    "print(\"Type of result:\", type(extracted_names_false))\n",
    "print(\"Head of extracted data:\\n\", extracted_names_false.head(7))\n",
    "print(\"\\nDataFrame Info (to see dtype):\")\n",
    "extracted_names_false.info()\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# --- 2. Using expand=True ---\n",
    "# When expand=True and there are multiple capturing groups,\n",
    "# it returns a DataFrame with a column for each captured group.\n",
    "extracted_names_true = df_titanic['Name'].str.extract(name_pattern, expand=True)\n",
    "\n",
    "# Rename columns for clarity\n",
    "extracted_names_true.columns = ['Surname', 'FirstName']\n",
    "\n",
    "print(\"\\n--- Result with expand=True ---\")\n",
    "print(\"Type of result:\", type(extracted_names_true))\n",
    "print(\"Head of extracted data:\\n\", extracted_names_true.head(7))\n",
    "print(\"\\nDataFrame Info (to see dtypes):\")\n",
    "extracted_names_true.info()\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# --- Add to original DataFrame for demonstration ---\n",
    "df_titanic[['Surname', 'FirstName']] = df_titanic['Name'].str.extract(name_pattern, expand=True)\n",
    "print(\"\\n--- df_titanic with new 'Surname' and 'FirstName' columns ---\")\n",
    "print(df_titanic[['Name', 'Surname', 'FirstName']].head(7))\n",
    "print(\"-\" * 70)\n",
    "Explanation of expand\n",
    "expand=False:\n",
    "\n",
    "Output Type: Returns a Pandas Series.\n",
    "Content: If your regex has:\n",
    "One capturing group: The Series elements will be the string matched by that group.\n",
    "Multiple capturing groups (as in our example): The Series elements will be tuples, where each tuple\n",
    " contains the strings matched by the respective capturing groups in that row. If a group doesn't match\n",
    "   for a specific row, its corresponding element in the tuple will be NaN.\n",
    "dtype: The dtype of the Series will typically be object, as it's storing heterogeneous tuples.\n",
    "expand=True:\n",
    "\n",
    "Output Type: Returns a Pandas DataFrame.\n",
    "Content: Each capturing group in your regex becomes a separate column in the resulting DataFrame.\n",
    "dtype: Each column in the DataFrame will have a dtype inferred from the data it contains (e.g., object for strings, or float64 if NaNs are present and it was numeric). This is generally more useful for direct integration into your DataFrame.\n",
    "In most scenarios where you're extracting multiple pieces of information from a string using regex capturing groups, expand=True is the more convenient option as it directly provides a structured DataFrame ready for further use.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a407ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "explore str.split(), str.lstrip(), and str.rstrip() using the df_titanic['Name'] column.\n",
    "These are incredibly useful for cleaning and parsing string data.\n",
    "\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Define the URL for the Titanic dataset\n",
    "titanic_dataset_github_url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/refs/heads/master/titanic.csv\"\n",
    "\n",
    "# Read the CSV file into a Pandas DataFrame\n",
    "try:\n",
    "    df_titanic = pd.read_csv(titanic_dataset_github_url)\n",
    "    print(\"Titanic dataset loaded successfully!\")\n",
    "    print(\"\\n--- Original 'Name' Column Head ---\")\n",
    "    print(df_titanic['Name'].head(7))\n",
    "    print(\"-\" * 70)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading CSV file: {e}\")\n",
    "    print(\"Please check the URL or your internet connection.\")\n",
    "\n",
    "\n",
    "# --- 1. str.split() ---\n",
    "# Splits strings in the Series by a specified delimiter.\n",
    "# If expand=True, it returns a DataFrame with new columns for each part.\n",
    "# If expand=False (default), it returns a Series of lists.\n",
    "\n",
    "# Split the 'Name' column by the comma (',')\n",
    "name_parts = df_titanic['Name'].str.split(',', expand=True)\n",
    "\n",
    "# Rename the new columns for clarity\n",
    "name_parts.columns = ['LastName', 'OtherNameInfo']\n",
    "\n",
    "print(\"\\n--- Result of df_titanic['Name'].str.split(',', expand=True) ---\")\n",
    "print(name_parts.head(7))\n",
    "print(\"\\nNotice the leading space in 'OtherNameInfo' (e.g., ' Mr. Owen Harris')\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "\n",
    "# --- 2. str.lstrip() ---\n",
    "# Removes leading characters (from the left side of the string).\n",
    "# By default, removes leading whitespace. You can also specify characters to remove.\n",
    "\n",
    "# Let's remove the leading space from the 'OtherNameInfo' column created above\n",
    "cleaned_other_name_info = name_parts['OtherNameInfo'].str.lstrip()\n",
    "\n",
    "print(\"\\n--- Result of name_parts['OtherNameInfo'].str.lstrip() ---\")\n",
    "print(pd.DataFrame({\n",
    "    'Original_OtherNameInfo': name_parts['OtherNameInfo'].head(7),\n",
    "    'Lstripped_OtherNameInfo': cleaned_other_name_info.head(7)\n",
    "}))\n",
    "print(\"\\nNotice the leading space is gone from 'Lstripped_OtherNameInfo'\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Example: Removing specific characters from the left\n",
    "sample_text_lstrip = pd.Series([\"###Hello World\", \"##Another#\"])\n",
    "print(\"\\n--- str.lstrip() with specific characters ---\")\n",
    "print(f\"Original:\\n{sample_text_lstrip}\")\n",
    "print(f\"lstrip('#'):\\n{sample_text_lstrip.str.lstrip('#')}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "\n",
    "# --- 3. str.rstrip() ---\n",
    "# Removes trailing characters (from the right side of the string).\n",
    "# By default, removes trailing whitespace. You can also specify characters to remove.\n",
    "\n",
    "# Let's imagine we want to remove the trailing period from the 'Title' part (e.g., \"Mr.\")\n",
    "# First, let's extract the title (we did this in a previous example, re-doing for context)\n",
    "df_titanic['Title_with_Dot'] = df_titanic['Name'].str.extract(r'([A-Za-z]+)\\.', expand=False)\n",
    "\n",
    "# Now, rstrip the dot\n",
    "title_stripped = df_titanic['Title_with_Dot'].str.rstrip('.')\n",
    "\n",
    "print(\"\\n--- Result of df_titanic['Title_with_Dot'].str.rstrip('.') ---\")\n",
    "print(pd.DataFrame({\n",
    "    'Original_Title': df_titanic['Title_with_Dot'].head(7),\n",
    "    'Rstripped_Title': title_stripped.head(7)\n",
    "}))\n",
    "print(\"\\nNotice the trailing dot is gone from 'Rstripped_Title'\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Example: Removing specific characters from the right\n",
    "sample_text_rstrip = pd.Series([\"Hello World###\", \"#Another##\"])\n",
    "print(\"\\n--- str.rstrip() with specific characters ---\")\n",
    "print(f\"Original:\\n{sample_text_rstrip}\")\n",
    "print(f\"rstrip('#'):\\n{sample_text_rstrip.str.rstrip('#')}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Example: Using all three in sequence for a common cleaning task\n",
    "df_titanic['Processed_Name'] = df_titanic['Name'].str.split(',').str[0].str.strip()\n",
    "print(\"\\n--- Combined Example: Extracting Cleaned Last Name ---\")\n",
    "print(df_titanic[['Name', 'Processed_Name']].head(7))\n",
    "print(\"Here, we split by comma, take the first part, and then strip any leading/trailing whitespace.\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "'''\n",
    "Key Points:\n",
    ".str Accessor: All these methods are part of the .str accessor, which is necessary to apply string operations to entire Pandas Series.\n",
    "str.split(delimiter, expand=True/False):\n",
    "Breaks a string into a list of substrings based on a delimiter.\n",
    "expand=True is incredibly useful for parsing structured strings into new columns directly.\n",
    "expand=False (default) returns a Series where each element is a Python list of the split parts. You'd then typically use .str[index] to access specific parts.\n",
    "str.lstrip(chars=None): Removes characters from the left (leading) side of the string.\n",
    "If chars is None (default), it removes all leading whitespace characters (spaces, tabs, newlines).\n",
    "If chars is specified (e.g., '#', '., '), it removes any combination of those characters from the left until a character not in chars is encountered.\n",
    "str.rstrip(chars=None): Removes characters from the right (trailing) side of the string.\n",
    "Behaves identically to lstrip(), but operates on the right side.\n",
    "str.strip(chars=None): Removes characters from both the left and right sides. This is often used for general whitespace cleaning.\n",
    "These methods are fundamental for text preprocessing and feature engineering in Pandas.\n",
    "\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
